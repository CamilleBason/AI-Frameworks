{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Q_Learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkS8-zF0IurD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wikistat/AI-Frameworks/blob/master/IntroductionDeepReinforcementLearning/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgByJnenIurD"
      },
      "source": [
        "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Introduction to Deep Reinforcement Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqcrl13SIurE"
      },
      "source": [
        "<center>\n",
        "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
        "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
        "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
        "    \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7EaWwceIurE"
      },
      "source": [
        "# Part 1a: Q-Learning\n",
        "The objectives of this notebook are the following : \n",
        "\n",
        "* As a reminder, implement Q-iteration and Q-Learning on simple Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwMmSfkkIurE"
      },
      "source": [
        "# Files & Data (Google Colab)\n",
        "\n",
        "If you're running this notebook on Google colab, you do not have access to the `solutions` folder you get by cloning the repository locally. \n",
        "\n",
        "The following lines will allow you to build the folders and the files you need for this TP.\n",
        "\n",
        "**WARNING 1** Do not run this line locally.\n",
        "**WARNING 2** The magic command `%load` does not work on google colab, you will have to copy-paste the solution on the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM8uszIrIurF"
      },
      "source": [
        "! mkdir solution\n",
        "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/hard_coded_policy.py\n",
        "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/q_iteration.py\n",
        "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/optimal_policy.py\n",
        "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/q_learning.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj9cOV4pIurF"
      },
      "source": [
        "# Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY3j7rhJIurF"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvtabuXnIurF"
      },
      "source": [
        "# Markov Decision Process\n",
        "\n",
        "## Definition\n",
        "\n",
        "We will first define a simple Markov process on which we will apply the Q-learning algorithm.\n",
        "\n",
        "Here is an illustration of the MDP that we will define.\n",
        "\n",
        "![images](https://github.com/wikistat/AI-Frameworks/blob/master/IntroductionDeepReinforcementLearning/images/mdp.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-_OiWSIurG"
      },
      "source": [
        "### Transition probabilities\n",
        "\n",
        "We first define the different **transition probabilities** for each $(s,a,s')$ combination where\n",
        "* $s$ is the `from_state`\n",
        "* $a$ is the `action` taken\n",
        "* $s$ is the `to_state`\n",
        "\n",
        "We store the **transition probabilities** within a python list and use pandas to visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE3M9NfXIurG"
      },
      "source": [
        "transition_probabilities = [\n",
        "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], \n",
        "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "        [None, [0.8, 0.1, 0.1], None],\n",
        "    ]\n",
        "\n",
        "transition_probabilities_df = pd.DataFrame(transition_probabilities).rename_axis('Actions', axis=1)\n",
        "transition_probabilities_df.index.name=\"State\"\n",
        "transition_probabilities_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeR37eJfIurG"
      },
      "source": [
        "### Rewards \n",
        "\n",
        "We also define the **rewards** for each $(s,a,s')$ combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a8BkkbEIurG"
      },
      "source": [
        "rewards = [\n",
        "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
        "    ]\n",
        "\n",
        "rewards_df = pd.DataFrame(rewards).rename_axis('Actions', axis=1)\n",
        "rewards_df.index.name=\"State\"\n",
        "rewards_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD_Y916MIurG"
      },
      "source": [
        "### Actions\n",
        "\n",
        "And the list of possible **actions** that can be taken at each state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgij7oE0IurG"
      },
      "source": [
        "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
        "\n",
        "possible_actions_df = pd.DataFrame([[x] for x in possible_actions], columns=[\"List of possible actions\"])\n",
        "possible_actions_df.index.name=\"State\"\n",
        "possible_actions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYAOx1KVIurG"
      },
      "source": [
        "## Class environment\n",
        "\n",
        "Finally we define now a class that will act as a Gym environment. \n",
        "\n",
        "* The environement is the MDP.\n",
        "* The observation is the current step.\n",
        "* The available actions are the three actions we previously defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5PWFpyuIurG"
      },
      "source": [
        "class MDPEnvironment(object):\n",
        "    def __init__(self, start_state=0):\n",
        "        self.start_state=start_state\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        self.state = self.start_state\n",
        "    def step(self, action):\n",
        "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
        "        reward = rewards[self.state][action][next_state]\n",
        "        self.state = next_state\n",
        "        self.total_rewards += reward\n",
        "        return self.state, reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLnHpoqWIurG"
      },
      "source": [
        "## Hard Coded Policy\n",
        "\n",
        "Let's first implement a random policy, as a baseline we want to improve.\n",
        "\n",
        "We run this policy one thousand of time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMeGWBU7IurG"
      },
      "source": [
        "def policy_random(state):\n",
        "    return np.random.choice(possible_actions[state])\n",
        "\n",
        "\n",
        "def run_episode(policy, n_steps, start_state=0):\n",
        "    env = MDPEnvironment()\n",
        "    for step in range(n_steps):\n",
        "        action = policy(env.state)\n",
        "        state, reward = env.step(action)\n",
        "    return env.total_rewards\n",
        "\n",
        "\n",
        "all_score = []\n",
        "for episode in range(1000):\n",
        "    all_score.append(run_episode(policy_random, n_steps=100))\n",
        "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_score), np.std(all_score), np.min(all_score), np.max(all_score)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnVxOKZIIurG"
      },
      "source": [
        "**Exercise** Which policy would be the safest? The riskier? Implement them and test them. What can you say about their results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9DzDyuOIurG"
      },
      "source": [
        "# %load solutions/hard_coded_policy.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZq2W8BcIurG"
      },
      "source": [
        "## Q-value iteration algorithm\n",
        "\n",
        "Let's know try to find the best policy! <br>\n",
        "Because we know all the **transition probabilities** and **reward values** for each $(s,a,s')$ combination we can calculate this best policy using the **Q-iteration algorithm**\n",
        "\n",
        "$$Q_{k+1}(s,a) \\leftarrow  \\sum_{s'}P^a_{s,s'}\\big[ R(s,a,s') + \\gamma \\cdot max_{a'}~Q_k(s',a') \\big]$$\n",
        "\n",
        "Let's first instantiate the Q-values table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er4VMN2eIurG"
      },
      "source": [
        "n_states = 3\n",
        "n_actions = 3\n",
        "gamma = 0.99  #<-- The discount rate\n",
        "q_values = np.full((n_states, n_actions), -np.inf) \n",
        "for state, action in enumerate(possible_actions):\n",
        "    q_values[state][action]=0\n",
        "q_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpjrgkSdIurG"
      },
      "source": [
        "**Exercise**: Implement the Q-iteration algorithm to find the optimal Q-values for each action-state couple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrZYjoMyIurG"
      },
      "source": [
        "# %load solutions/q_iteration.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IU12jkPIurG"
      },
      "source": [
        "We are now able to follow the best policy in each state!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB7Q3UrZIurG"
      },
      "source": [
        "optimal_action_per_state = np.argmax(q_values,axis=1)\n",
        "optimal_action_per_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LOO6okoIurG"
      },
      "source": [
        "**Exercise**: Implement the best policy from the q_values you estimated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2k1D48-IurG"
      },
      "source": [
        "# %load solutions/optimal_policy.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3SpV3agIurG"
      },
      "source": [
        "all_totals = []\n",
        "for episode in range(1000):\n",
        "    all_totals.append(run_episode(optimal_policy, n_steps=100))\n",
        "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SXYuasGIurG"
      },
      "source": [
        "## Q-Learning iteration algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHz-VD5BIurG"
      },
      "source": [
        "Now, let us implement Q-learning algorithm to learn a better policy!\n",
        "\n",
        "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. \n",
        "Once it has accurate Q-Value estimates, then the optimal policy consists of choosing the action that has the highest Q-Value (i.e., the greedy policy).\n",
        "\n",
        "We first initiate:\n",
        "* the different parameters (learning_rate $\\alpha$ and the discount rate $\\gamma$}\n",
        "* The number of steps to run\n",
        "* The exploration policy (random one)\n",
        "* The Q-values tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRUB-1nMIurG"
      },
      "source": [
        "n_states = 3\n",
        "n_actions = 3\n",
        "n_steps = 2000\n",
        "alpha = 0.01  #<-- Learning Rate\n",
        "gamma = 0.99  #<-- The discount rate\n",
        "\n",
        "\n",
        " \n",
        "exploration_policy = policy_random #<-- Policy that we will play during exploration\n",
        "q_values = np.full((n_states, n_actions), -np.inf) #<-- Policy that we will be updated\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    q_values[state][actions]=0\n",
        "q_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD02Tp6pIurG"
      },
      "source": [
        "**Exercise**\n",
        "Run *n_steps* over the MDP and update the Q-values table at each step according to the Q-learning iteration algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rOYGZeZIurG"
      },
      "source": [
        "# %load solutions/q_learning.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N6qftr1IurG"
      },
      "source": [
        "**Question** Run the algorithm over 20000 steps and observe the optimal action per state below. Run it, once again, over 20000 episodes. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JykGjHFIurG"
      },
      "source": [
        "optimal_action_per_state = np.argmax(q_values,axis=1)\n",
        "optimal_action_per_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXOy7RGGIurG"
      },
      "source": [
        "def optimal_policy(state):\n",
        "    return optimal_action_per_state[state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBVrkx16IurG"
      },
      "source": [
        "Verify its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJuIR0GEIurG"
      },
      "source": [
        "all_totals = []\n",
        "for episode in range(1000):\n",
        "    all_totals.append(run_episode(optimal_policy, n_steps=100))\n",
        "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
