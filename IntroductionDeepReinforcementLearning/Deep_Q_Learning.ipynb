{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Introduction to Deep Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Deep Q-Network\n",
    "The objectives of this noteboks are the following : \n",
    "\n",
    "* Implement DQN to solve gridworld (a pacman-like game).\n",
    "* [OPTIONAL] Implement D3QN on the same game.\n",
    "* [OPTIONAL] Implement those algorithm to solve real pacman game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import collections\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# To plot figures and animations\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions enable to build a video from a list of images. <br>\n",
    "They will be used to build video of the game you will played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=400):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World environment\n",
    "\n",
    "The GridWorld environment will be used all along this notebook. <br>\n",
    "This environment has been developed by Arthur Juliani and original code can be found on his [github](https://github.com/awjuliani/DeepRL-Agents/blob/master/gridworld.py).\n",
    "\n",
    "We have a 5x5 blocs where blue, green and red squares. In the environment:\n",
    "\n",
    "* the **agent** controls a *blue* square\n",
    "* the goal is to navigate to the *green* squares (**reward** +1) \n",
    "* and avoiding the *red* squares (**reward** -1).\n",
    "\n",
    "This game can be seen as a simplify pacman game. The *blue* square is the pacman, the *green* square are the pellets and give positive rewards and the *red* square are the ghost and give negative rewards.<br>\n",
    "*Green* and *red* square do not move.\n",
    "\n",
    "\n",
    "### Observation\n",
    "\n",
    "The observation is the image itself which is a 84x84x3 images.\n",
    "\n",
    "### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Go Up\n",
    "1 | Go Down\n",
    "2 | Go Left\n",
    "3 | Go Right\n",
    "\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every green square taken and -1 when a red square is taken\n",
    "\n",
    "### End epsiode\n",
    "There are no condition limit for an episode to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initiate an environment and display the first state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADD0lEQVR4nO3dMQrCABBFwUS8/5XXAxhIZfaBM6UWRuGxYPPPmTmAntf2AwDXxAlR4oQocUKUOCHqffO+v3Lh986rF11OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1N0E4J7zchXtmY9e+2S2zPTWLl1OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFCVHYCcHWGb3MN7o/3B3sjfLtcTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQlR2AnDV3+4PHsfml9/82bd/9SsuJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEGWfs2ZWx0EJcTkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRJgBrLADumO0H+OZyQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTorITgDPBTTZ4kMsJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IupsAPB95CuCLywlR4oQocUKUOCFKnBAlToj6ALzkEdrWyHvbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Let's play a game manually to understand how it works. <br>\n",
    "Fill the `actions_list` with steps to move the blue square in order to catch green square and win points\n",
    "\n",
    "The last line of code `HTML(plot_animation(frames).to_html5_video())` enable to display the video of the game from the list of state produced during the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "cum_reward=0\n",
    "actions_list=[]\n",
    "for step in actions_list:\n",
    "    state, reward , end = env.step(step)\n",
    "    frames.append(state)\n",
    "    cum_reward+=reward\n",
    "print(cum_reward)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP Q Learning on *Gridworld*\n",
    "\n",
    "The objective of this section is to implement a **Deep Q-learning** tha will be able to play (correctly) gridworld environment.\n",
    "\n",
    "For that 3 python class will be required:\n",
    "\n",
    "* `ExperienceReplay`: A class that implement the **Experience Replay Buffer**.  This class is already implemented.\n",
    "* `Qnetwork`: A class that will enable \n",
    "* `DQN`: A class that will enable \n",
    "\n",
    "\n",
    "All the instructions of this section are in this notebook belows. \n",
    "\n",
    "However you will have the possibility to \n",
    "* work with the scripts DQN.py and DQN_test.py that can be found in the `IntroductionDeepReinforcementLearning`folder\n",
    "* work with the codes in cells of this notebok. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer\n",
    "\n",
    "\n",
    "The **Experience Replay Buffer** is where all the agent's experience will be stored and where *batch* will be generate from in order to train the *Q network*  \n",
    "\n",
    "The `ExperienceReplay` class below is the implementation of this function. \n",
    "\n",
    "The `buffer_size` argument represent the number of element that are kept in memory (in the `buffer`). <br>\n",
    "Even if 10Milions of games have been played, the `Experience Replay` will kept only the last `buffer_size` argument in memory. <br>\n",
    "Hence at the beginning the first batch of targets will be composed of randomly played experience. And during training, the probability that batch of targets will be compose of experience playe in exploitation mode will increase.\n",
    "\n",
    "The `add` method will add element on the `buffer `.\n",
    "\n",
    "The `sample`method will generate a sample of `size`element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # Extend the stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # Keep the last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.buffer), size=size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output, (size, -1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a simple example on how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[[91]\n",
      " [93]\n",
      " [97]\n",
      " [96]\n",
      " [94]]\n"
     ]
    }
   ],
   "source": [
    "# Instanciate an experience replay buffer with buffer_size 10\n",
    "experience_replay = ExperienceReplay(buffer_size=10)\n",
    "# Add list of 100 integer in the buffer\n",
    "experience_replay.add(list(range(100)))\n",
    "# Check that it keeps only the las 10 element\n",
    "print(experience_replay.buffer)\n",
    "# Randomly sample 5 element from the buffer\n",
    "sample = experience_replay.sample(5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q network\n",
    "\n",
    "In **Q-learning** all the *Q-Values* are stored in a *Q-table*. \n",
    "The optimal value can be learn by playing game and updating the Q-table with the following formula.\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a')$$\n",
    "$$Q_{k+1}(s,a)\\leftarrow(1-a)Q_k(s,a)+\\alpha[target]$$\n",
    "\n",
    "if the combinations of states and actions are too large, the memory and the computation requirement for the *Q-table* will be too high.\n",
    "\n",
    "Hence, in **Deep Q-learning** we use a function to generate the approximation of the *Q-value* rather than remembering the solutions. <br>\n",
    "As the input of the function, i.e, the *state*, is an image, we will use a *convolutional neural network* model to approximate the Q values.\n",
    "\n",
    "Later, we will generate targets from exeperiences and train this CNN.\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a')$$\n",
    "$$\\theta_{k+1} \\leftarrow \\theta_k - \\alpha\\nabla_{\\theta}\\mathbb{E}_{s\\sim'P(s'|s,a)} [(Q_{\\theta}(s,a)-target(s'))^2]_{\\theta=\\theta_k} $$\n",
    "\n",
    "The `Qnetwork` class below defines the architecture of this *convolutional neural network*.\n",
    "\n",
    "**Exercise** \n",
    "\n",
    "The architecture of the *cnn* as been set for you, as it can requires various iterations to defined it and can takes some times. <br>\n",
    "It is compose of 4 convolutional layers and two dens layer. <br>\n",
    "\n",
    "However, the shape of the input as well as the number of neurons and the activation function  of the last layer are not filled.<br>\n",
    "Fill the gap so that this network can be use to approximate *Q-values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-db768d054d99>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-db768d054d99>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    self.inputs = kl.Input(shape=??, name=\"main_input\")\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Qnetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = kl.Input(shape=??, name=\"main_input\")\n",
    "\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[8, 8],\n",
    "            strides=[4, 4],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv1\")(self.inputs)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv2\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv3\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[7, 7],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv4\")(self.model)\n",
    "\n",
    "        self.model = kl.Flatten()(self.model)\n",
    "        self.model = kl.Dense(256, activation=\"relu\")(self.model)\n",
    "        self.model = kl.Dense(??, activation=??)(self.model)\n",
    "        self.model = km.Model(self.inputs, self.model)\n",
    "        self.model.compile(\"adam\", \"mse\")\n",
    "        self.model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Qnetwork_class.py\n",
    "class Qnetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = kl.Input(shape=[84, 84, 3], name=\"main_input\")\n",
    "\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[8, 8],\n",
    "            strides=[4, 4],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv1\")(self.inputs)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv2\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv3\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[7, 7],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv4\")(self.model)\n",
    "\n",
    "        self.model = kl.Flatten()(self.model)\n",
    "        self.model = kl.Dense(256, activation=\"relu\")(self.model)\n",
    "        self.model = kl.Dense(4, activation=\"linear\")(self.model)\n",
    "        self.model = km.Model(self.inputs, self.model)\n",
    "        self.model.compile(\"adam\", \"mse\")\n",
    "        self.model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init a model in order to display it's summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      [(None, 84, 84, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 20, 20, 32)        6176      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 1, 1, 512)         1606144   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 1,814,436\n",
      "Trainable params: 1,814,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_qn = Qnetwork()\n",
    "main_qn.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class\n",
    "\n",
    "\n",
    "The `DQN` class contains the implementation of the **Deep Q-Learning** algorithm. The code is incomplete and you will have to fill it!. \n",
    "\n",
    "**GENERAL INSTRUCTION**:\n",
    "\n",
    "* Read the init of the `DQN` class. \n",
    "    * Various variable are set with their definition, make sure you understand all of its.\n",
    "    * The *game environment*, the *experience replay buffer* and both *main Q-network* and *target Q-network* are initialised.\n",
    "* Read the `train` method. It contains the main code corresponding to the **pseudo code** below. YOU DO NOT HAVE TO MODIFY IT! But make sure you understand it.\n",
    "* The `train` method use methods that are not implemented. \n",
    "    * You will have to complete the code of 4 functions. (read instruction of each exercise below)\n",
    "    * After the cell of the `DQN` class code below there are **test cells** for each of these exercices. <br>\n",
    "    This cell should be executed after each exercice. This cell will check that the function you implemented take input and output in the desired format. <br> DO NOT MODIFY this cell. They will work if you're code is good <br> **Warning** The test celle does not guarantee that your code is correct. It just test than input and output are in the good format.\n",
    "\n",
    "\n",
    "#### Pseudo code \n",
    "Set both *main Q-network* and *target Q-network* equals and initiate *prob_random*\n",
    "While you didn't reach the expected *goal* reward or the *max_num_episode* allow to be played:\n",
    "* Play a complete episode (**Exercise 2 & 3**)\n",
    "* Store this episode in the buffer.\n",
    "* If you have played more than *min_pre_train_episode* randomly:\n",
    "    * Decrease the probability to play random\n",
    "    * Every *train_frequency* episode:\n",
    "        * Train *num_epochs* over a batch of targets (**Exercice 4**)\n",
    "        * Update weight of the *target Q-network*  so that their equals to the *main Q-network* (**Exercise 1**)\n",
    "\n",
    "    \n",
    "**Exercise 1**:  Implement `update_target_graph`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This function update the weight of `target_qn` with those of `main_qn`.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Tips** Use `set_weights`and `get_weights`method of [keras api](https://keras.io/api/layers/)\n",
    "    \n",
    "**Exercise 2**:  Implement `choose_action`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; This method chooses an action in *eploration* or *eploitation* mod according to the following rules:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> if less than `min_pre_train_episodes`has been played, choose action randomly<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> else play randomly with probability `prob_random` else exploit the model.\n",
    "\n",
    "\n",
    "**Exercise 3**:  Implement `run_one_episode` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; This method:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> initiate a new environment<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> play a game until it's done OR until `max_num_step` is reached.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> all the experiences  are stored and return.\n",
    "\n",
    "**Exercise 4**:  Implement `generate_target_q`<br>\n",
    "This method is used within the `train_one_step` method (which is already implemented).This method:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Generate a batch of data for training using the `experience_replay` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Generate the targets from this batch using `generate_target_q` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Train the model using these targets. <br>\n",
    "<br> \n",
    "The `generate_target_q` is not implemented so you have to do it!<br>\n",
    "You have to generate targets according to the formula below <br>\n",
    "\n",
    " \n",
    "    \n",
    "#### Separated Target network\n",
    "Here is how the target is supposed to be computed in order to train or Deep-Q network.\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a';\\theta) $$\n",
    "\n",
    "\n",
    "We build a deep network to learn the values of Q but its target values are changing as we know things better. As shown below, the target values for Q depends on Q itself, we are chasing a non-stationary target. So we use a different network to generate the target, that will be updated from time to time\n",
    "\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a';\\theta_{target}) $$\n",
    "\n",
    "# TODO give tips for train_gameover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64  # How many experiences to use for each training step\n",
    "        self.train_frequency = 5  # How often you update the network\n",
    "        self.num_epochs = 20  # How many epochs to train when updating the network\n",
    "        self.y = 0.99  # Discount factor\n",
    "        self.prob_random_start = 0.6  # Starting chance of random action\n",
    "        self.prob_random_end = 0.1  # Ending chance of random action\n",
    "        self.annealing_steps = 1000.  # Steps of training to reduce from start_e -> end_e\n",
    "        self.max_num_episodes = 10000  # Max number of episodes you are allowes to played to train the game\n",
    "        self.min_pre_train_episodes = 100  # Number of episodes played with random actions before to start training.\n",
    "        self.max_num_step = 50  # Maximum allowed episode length\n",
    "        self.goal = 15 # Number of rewards we want to achieve while playing a game.\n",
    "\n",
    "        # Set env\n",
    "        self.env = gameEnv(partial=False, size=5)\n",
    "\n",
    "        # Reset everything from keras session\n",
    "        K.clear_session()\n",
    "\n",
    "        # Setup our Q-networks\n",
    "        self.main_qn = Qnetwork()\n",
    "        self.target_qn = Qnetwork()\n",
    "\n",
    "        # Setup our experience replay\n",
    "        self.experience_replay = ExperienceReplay()\n",
    "\n",
    "    def update_target_graph(self):\n",
    "        # TODO\n",
    "        return\n",
    "\n",
    "    def choose_action(self, state, prob_random, num_episode):\n",
    "        # TODO\n",
    "        return action\n",
    "\n",
    "    def run_one_episode(self, num_episode, prob_random):\n",
    "        # TODO\n",
    "        return experiences_episode\n",
    "\n",
    "    def generate_target_q(self, train_state, train_action, train_reward, train_next_state, train_done):\n",
    "        # TODO\n",
    "        return target_q\n",
    "\n",
    "    def train_one_step(self):\n",
    "        # Train batch is [[state,action,reward,next_state,done],...]\n",
    "        train_batch = self.experience_replay.sample(self.batch_size)\n",
    "\n",
    "        # Separate the batch into numpy array for each compents\n",
    "        train_state = np.array([x[0] for x in train_batch])\n",
    "        train_action = np.array([x[1] for x in train_batch])\n",
    "        train_reward = np.array([x[2] for x in train_batch])\n",
    "        train_next_state = np.array([x[3] for x in train_batch])\n",
    "        train_done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "        # Generate target Q\n",
    "        target_q = self.generate_target_q(\n",
    "            train_state=train_state,\n",
    "            train_action=train_action,\n",
    "            train_reward=train_reward,\n",
    "            train_next_state=train_next_state,\n",
    "            train_done=train_done\n",
    "        )\n",
    "\n",
    "        # Train the main model\n",
    "        loss = self.main_qn.model.train_on_batch(train_state, target_q)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Make the networks equal\n",
    "        self.update_target_graph()\n",
    "\n",
    "        # We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "        # we will begin reducing the probability of acting randomly, and instead\n",
    "        # take the actions that our Q network suggests\n",
    "        prob_random = self.prob_random_start\n",
    "        prob_random_drop = (self.prob_random_start - self.prob_random_end) / self.annealing_steps\n",
    "\n",
    "        # Init variable\n",
    "        num_steps = []  # Tracks number of steps per episode\n",
    "        rewards = []  # Tracks rewards per episode\n",
    "        print_every = 50  # How often to print status\n",
    "        losses = [0]  # Tracking training losses\n",
    "        num_episode = 0\n",
    "\n",
    "        while True:\n",
    "            # Run one episode\n",
    "            experiences_episode = self.run_one_episode(num_episode, prob_random)\n",
    "\n",
    "            # Save the episode in the replay buffer\n",
    "            self.experience_replay.add(experiences_episode)\n",
    "\n",
    "            # If we have play enoug episode. Start the training\n",
    "            if num_episode > self.min_pre_train_episodes:\n",
    "\n",
    "                # Drop the probability of a random action if wi didn't reach the prob_random_end value\n",
    "                if prob_random > self.prob_random_end:\n",
    "                    prob_random -= prob_random_drop\n",
    "\n",
    "                # Every train_frequency iteration, train the model\n",
    "                if num_episode % self.train_frequency == 0:\n",
    "                    for num_epoch in range(self.num_epochs):\n",
    "                        loss = self.train_one_step()\n",
    "                        losses.append(loss)\n",
    "\n",
    "                    # Update the target model with values from the main model\n",
    "                    self.update_target_graph()\n",
    "\n",
    "            # Increment the episode\n",
    "            num_episode += 1\n",
    "            num_steps.append(len(experiences_episode))\n",
    "            rewards.append(sum([e[2] for e in experiences_episode]))\n",
    "\n",
    "            # Print Info\n",
    "            if num_episode % print_every == 0:\n",
    "                # datetime object containing current date and time\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                mean_loss = np.mean(losses[-(print_every * self.num_epochs):])\n",
    "                print(\"{} - Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "                    dt_string, num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "\n",
    "            # Stop Condition\n",
    "            if np.mean(rewards[-print_every:]) >= self.goal:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                mean_loss = np.mean(losses[-(print_every * self.num_epochs):])\n",
    "                print(\"{} - Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "                    dt_string, num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "                print(\"Training complete because we reached goal rewards.\")\n",
    "                break\n",
    "            if num_episode > self.max_num_episodes:\n",
    "                print(\"Training Stop because we reached max num of episodes\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `update_target_graph`**\n",
    "\n",
    "This test ensure that\n",
    "* all layers weights of both *main Q-network* and *target Q-network* are **different** at initialisation\n",
    "* all layers weights of both *main Q-network* and *target Q-network* are **equal** after executing `update_target_graph`method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 Update weight copy weight\n",
    "dqn = DQN()\n",
    "for target_layer_weight, main_layer_weight in zip(dqn.target_qn.model.get_weights(), dqn.main_qn.model.get_weights()):\n",
    "    if len(target_layer_weight.shape)>1:\n",
    "        assert not(np.all(target_layer_weight == main_layer_weight))\n",
    "\n",
    "dqn.update_target_graph()\n",
    "for target_layer_weight, main_layer_weight in zip(dqn.target_qn.model.get_weights(), dqn.main_qn.model.get_weights()):\n",
    "    if len(target_layer_weight.shape)>1:\n",
    "        assert np.all(target_layer_weight == main_layer_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `choose_action`**\n",
    "\n",
    "This test can't be considered as a real test. <br>\n",
    "Indeed, if the action are play randomly we can't expect a fixed results. \n",
    "\n",
    "However, if your function is implemented correctly these test should word most of the time:\n",
    "\n",
    "* If `num_episode`=99 (below `min_pre_train_epsiode`=100) -> play randomly even if `prob_random` = 0 \n",
    "    * Over 100 play, each action should appears various time\n",
    "* If `prob_random` = 1 -> play randomly even if  `num_episode`=101 (above `min_pre_train_epsiode`=100) \n",
    "    * Over 100 play, each action should appears various time\n",
    "* If `prob_random` = 0 and `num_episode`=101 (above `min_pre_train_epsiode`=100) -> play in exploit mode \n",
    "    * The same action is choosen all the time.\n",
    "* If `prob_random` = 0.5 and `num_episode`=101 (above `min_pre_train_epsiode`=100) -> play both exploration and exploit mode randomly. \n",
    "    * All action sould be seen, but the action choosen in exploit mode is always the same and should be choosen more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "state = dqn.env.reset()\n",
    "# Random action if less than min_pre_train_episode has been played\n",
    "actions = [dqn.choose_action(state=state,num_episode=99, prob_random=0) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "print(count_action)\n",
    "assert count_action[0]>15\n",
    "assert count_action[1]>15\n",
    "assert count_action[2]>15\n",
    "assert count_action[3]>15\n",
    "\n",
    "# Random action if we play more than min_pre_train_episode and prob_random is 1\n",
    "actions = [dqn.choose_action(state=state,num_episode=101, prob_random=1) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "print(count_action)\n",
    "assert count_action[0]>15\n",
    "assert count_action[1]>15\n",
    "assert count_action[2]>15\n",
    "assert count_action[3]>15\n",
    "\n",
    "# Best action according to model if we play more than min_pre_train_episode and prob_random is 0\n",
    "actions = [dqn.choose_action(state=state,num_episode=101, prob_random=0) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "print(count_action)\n",
    "assert(len(set(actions)))==1\n",
    "main_action = list(set(actions))[0]\n",
    "\n",
    "actions = [dqn.choose_action(state=state,num_episode=101, prob_random=0.5) for _ in range(100)]\n",
    "count_action = collections.Counter(actions)\n",
    "assert(len(set(actions)))==4\n",
    "print(count_action)\n",
    "assert sorted(count_action.items(), key=lambda x : x[1])[-1][0]==main_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `run_one_episode`**\n",
    "\n",
    "The `run_one_episode` play a complete episode.\n",
    "\n",
    "* The method return a list of experiences. Each experience is a list that contains :\n",
    " * A *state*: an image of shape (84,84,3)\n",
    " * An *action*: an integer\n",
    " * A *reward*: a float\n",
    " * The *nex_state*: an image of shape (84,84,3)\n",
    " * A boolean that indicate if the game is over or not after this action.\n",
    "* The experiences list can't have more experience than `max_num_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "experiences_episode = dqn.run_one_episode(num_episode=200,\n",
    "                          prob_random=1)\n",
    "\n",
    "for experience in experiences_episode:\n",
    "    state, action, reward, next_state, done = experience\n",
    "\n",
    "    assert state.shape == (84, 84, 3)\n",
    "    assert type(action) is int\n",
    "    assert type(reward) is float\n",
    "    assert next_state.shape == (84, 84, 3)\n",
    "    assert type(done) is bool\n",
    "assert len(experiences_episode)<=dqn.max_num_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test `generate_target_q`**\n",
    "\n",
    "This method generate targets of q values.\n",
    "\n",
    "In this test we set the `batch_size`value is equal to 2. Hence the function take as an input: \n",
    "* train_state : An array of size (2,83,83,4)\n",
    "* train_action : An array of size (2,1)\n",
    "* train_reward  : An array of size (2,1)\n",
    "* train_next_state : An array of size (2,83,83,4)\n",
    "* train_done : An array of size (2,1)\n",
    "\n",
    "And return as an output an Array of size (2,1), which is a target for each input of the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "dqn.batch_size=2\n",
    "state = np.expand_dims(dqn.env.reset(), axis=0)\n",
    "target_q = dqn.generate_target_q(\n",
    "    train_state = np.vstack([state,state]),\n",
    "    train_action = np.array([0,0]),\n",
    "    train_reward = np.array([1.0,2.0]),\n",
    "    train_next_state = np.vstack([state,state]),\n",
    "    train_done = np.array([1, 1])\n",
    ")\n",
    "\n",
    "assert target_q.shape == (2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/DQN_class.py\n",
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64  # How many experiences to use for each training step\n",
    "        self.train_frequency = 5  # How often you update the network\n",
    "        self.num_epochs = 20  # How many epochs to train when updating the network\n",
    "        self.y = 0.99  # Discount factor\n",
    "        self.prob_random_start = 0.6  # Starting chance of random action\n",
    "        self.prob_random_end = 0.1  # Ending chance of random action\n",
    "        self.annealing_steps = 1000.  # Steps of training to reduce from start_e -> end_e\n",
    "        self.max_num_episodes = 10000  # Max number of episodes you are allowes to played to train the game\n",
    "        self.min_pre_train_episodes = 100  # Number of episodes played with random actions before to start training.\n",
    "        self.max_num_step = 50  # Maximum allowed episode length\n",
    "        self.goal = 15 # Number of rewards we want to achieve while playing a game.\n",
    "\n",
    "        # Set env\n",
    "        self.env = gameEnv(partial=False, size=5)\n",
    "\n",
    "        # Reset everything from keras session\n",
    "        K.clear_session()\n",
    "\n",
    "        # Setup our Q-networks\n",
    "        self.main_qn = Qnetwork()\n",
    "        self.target_qn = Qnetwork()\n",
    "\n",
    "        # Setup our experience replay\n",
    "        self.experience_replay = ExperienceReplay()\n",
    "\n",
    "    def update_target_graph(self):\n",
    "        updated_weights = np.array(self.main_qn.model.get_weights())\n",
    "        self.target_qn.model.set_weights(updated_weights)\n",
    "\n",
    "    def choose_action(self, state, prob_random, num_episode):\n",
    "        if np.random.rand() < prob_random or \\\n",
    "                num_episode < self.min_pre_train_episodes:\n",
    "            # Act randomly based on prob_random or if we\n",
    "            # have not accumulated enough pre_train episodes\n",
    "            action = np.random.randint(self.env.actions)\n",
    "        else:\n",
    "            # Decide what action to take from the Q network\n",
    "            # First add one dimension to the netword to fit expected dimension of the network\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            action = np.argmax(self.main_qn.model.predict(state))\n",
    "        return action\n",
    "\n",
    "    def run_one_episode(self, num_episode, prob_random):\n",
    "        # Create an experience replay for the current episode.\n",
    "        experiences_episode = []\n",
    "\n",
    "        # Get the game state from the environment\n",
    "        state = self.env.reset()\n",
    "\n",
    "        done = False  # Game is complete\n",
    "        cur_step = 0  # Running sum of number of steps taken in episode\n",
    "\n",
    "        while cur_step < self.max_num_step and not done:\n",
    "            cur_step += 1\n",
    "            action = self.choose_action(\n",
    "                state=state,\n",
    "                prob_random=prob_random,\n",
    "                num_episode=num_episode\n",
    "            )\n",
    "\n",
    "            # Take the action and retrieve the next state, reward and done\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            # Setup the experience to be stored in the episode buffer\n",
    "            experience = [state, action, reward, next_state, done]\n",
    "\n",
    "            # Store the experience in the episode buffer\n",
    "            experiences_episode.append(experience)\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "\n",
    "        return experiences_episode\n",
    "\n",
    "    def generate_target_q(self, train_state, train_action, train_reward, train_next_state, train_done):\n",
    "        # Our predictions (actions to take) from the main Q network\n",
    "        target_q = self.main_qn.model.predict(train_state)\n",
    "\n",
    "        # Tells us whether game over or not\n",
    "        # We will multiply our rewards by this value\n",
    "        # to ensure we don't train on the last move\n",
    "        train_gameover = train_done == 0\n",
    "\n",
    "        # Q value of the next state based on action\n",
    "        target_q_next_state = self.target_qn.model.predict(train_next_state)\n",
    "        train_next_state_values = np.max(target_q_next_state[range(self.batch_size)], axis=1)\n",
    "\n",
    "        # Reward from the action chosen in the train batch\n",
    "        actual_reward = train_reward + (self.y * train_next_state_values * train_gameover)\n",
    "        target_q[range(self.batch_size), train_action] = actual_reward\n",
    "        return target_q\n",
    "\n",
    "    def train_one_step(self):\n",
    "        # Train batch is [[state,action,reward,next_state,done],...]\n",
    "        train_batch = self.experience_replay.sample(self.batch_size)\n",
    "\n",
    "        # Separate the batch into numpy array for each compents\n",
    "        train_state = np.array([x[0] for x in train_batch])\n",
    "        train_action = np.array([x[1] for x in train_batch])\n",
    "        train_reward = np.array([x[2] for x in train_batch])\n",
    "        train_next_state = np.array([x[3] for x in train_batch])\n",
    "        train_done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "        # Generate target Q\n",
    "        target_q = self.generate_target_q(\n",
    "            train_state=train_state,\n",
    "            train_action=train_action,\n",
    "            train_reward=train_reward,\n",
    "            train_next_state=train_next_state,\n",
    "            train_done=train_done\n",
    "        )\n",
    "\n",
    "        # Train the main model\n",
    "        loss = self.main_qn.model.train_on_batch(train_state, target_q)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Make the networks equal\n",
    "        self.update_target_graph()\n",
    "\n",
    "        # We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "        # we will begin reducing the probability of acting randomly, and instead\n",
    "        # take the actions that our Q network suggests\n",
    "        prob_random = self.prob_random_start\n",
    "        prob_random_drop = (self.prob_random_start - self.prob_random_end) / self.annealing_steps\n",
    "\n",
    "        # Init variable\n",
    "        num_steps = []  # Tracks number of steps per episode\n",
    "        rewards = []  # Tracks rewards per episode\n",
    "        print_every = 50  # How often to print status\n",
    "        losses = [0]  # Tracking training losses\n",
    "        num_episode = 0\n",
    "\n",
    "        while True:\n",
    "            # Run one episode\n",
    "            experiences_episode = self.run_one_episode(num_episode, prob_random)\n",
    "\n",
    "            # Save the episode in the replay buffer\n",
    "            self.experience_replay.add(experiences_episode)\n",
    "\n",
    "            # If we have play enoug episode. Start the training\n",
    "            if num_episode > self.min_pre_train_episodes:\n",
    "\n",
    "                # Drop the probability of a random action if wi didn't reach the prob_random_end value\n",
    "                if prob_random > self.prob_random_end:\n",
    "                    prob_random -= prob_random_drop\n",
    "\n",
    "                # Every train_frequency iteration, train the model\n",
    "                if num_episode % self.train_frequency == 0:\n",
    "                    for num_epoch in range(self.num_epochs):\n",
    "                        loss = self.train_one_step()\n",
    "                        losses.append(loss)\n",
    "\n",
    "                    # Update the target model with values from the main model\n",
    "                    self.update_target_graph()\n",
    "\n",
    "            # Increment the episode\n",
    "            num_episode += 1\n",
    "            num_steps.append(len(experiences_episode))\n",
    "            rewards.append(sum([e[2] for e in experiences_episode]))\n",
    "\n",
    "            # Print Info\n",
    "            if num_episode % print_every == 0:\n",
    "                # datetime object containing current date and time\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                mean_loss = np.mean(losses[-(print_every * self.num_epochs):])\n",
    "                print(\"{} - Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "                    dt_string, num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "\n",
    "            # Stop Condition\n",
    "            if np.mean(rewards[-print_every:]) >= self.goal:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                mean_loss = np.mean(losses[-(print_every * self.num_epochs):])\n",
    "                print(\"{} - Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "                    dt_string, num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "                print(\"Training complete because we reached goal rewards.\")\n",
    "                break\n",
    "            if num_episode > self.max_num_episodes:\n",
    "                print(\"Training Stop because we reached max num of episodes\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/11/2020 14:58:20 - Num episode: 50 Mean reward: 1.8400 Prob random: 0.6000, Loss: 0.0000\n",
      "08/11/2020 14:58:20 - Num episode: 100 Mean reward: 2.3800 Prob random: 0.6000, Loss: 0.0000\n",
      "08/11/2020 14:59:40 - Num episode: 150 Mean reward: 1.6600 Prob random: 0.5755, Loss: 0.8833\n",
      "08/11/2020 15:01:05 - Num episode: 200 Mean reward: 1.9600 Prob random: 0.5505, Loss: 0.4618\n",
      "08/11/2020 15:02:30 - Num episode: 250 Mean reward: 2.6000 Prob random: 0.5255, Loss: 0.3236\n",
      "08/11/2020 15:03:58 - Num episode: 300 Mean reward: 2.1800 Prob random: 0.5005, Loss: 0.2564\n",
      "08/11/2020 15:05:31 - Num episode: 350 Mean reward: 3.0400 Prob random: 0.4755, Loss: 0.2167\n",
      "08/11/2020 15:07:04 - Num episode: 400 Mean reward: 3.9800 Prob random: 0.4505, Loss: 0.0627\n",
      "08/11/2020 15:08:39 - Num episode: 450 Mean reward: 3.1800 Prob random: 0.4255, Loss: 0.0573\n",
      "08/11/2020 15:10:15 - Num episode: 500 Mean reward: 4.0400 Prob random: 0.4005, Loss: 0.0589\n",
      "08/11/2020 15:11:52 - Num episode: 550 Mean reward: 4.9400 Prob random: 0.3755, Loss: 0.0569\n",
      "08/11/2020 15:13:31 - Num episode: 600 Mean reward: 5.4200 Prob random: 0.3505, Loss: 0.0565\n",
      "08/11/2020 15:15:12 - Num episode: 650 Mean reward: 6.2400 Prob random: 0.3255, Loss: 0.0578\n",
      "08/11/2020 15:16:55 - Num episode: 700 Mean reward: 4.4200 Prob random: 0.3005, Loss: 0.0613\n",
      "08/11/2020 15:18:39 - Num episode: 750 Mean reward: 7.7400 Prob random: 0.2755, Loss: 0.0582\n",
      "08/11/2020 15:20:25 - Num episode: 800 Mean reward: 7.7000 Prob random: 0.2505, Loss: 0.0586\n",
      "08/11/2020 15:22:12 - Num episode: 850 Mean reward: 6.5400 Prob random: 0.2255, Loss: 0.0592\n",
      "08/11/2020 15:24:06 - Num episode: 900 Mean reward: 5.5600 Prob random: 0.2005, Loss: 0.0584\n",
      "08/11/2020 15:26:00 - Num episode: 950 Mean reward: 7.2200 Prob random: 0.1755, Loss: 0.0544\n",
      "08/11/2020 15:28:01 - Num episode: 1000 Mean reward: 11.3800 Prob random: 0.1505, Loss: 0.0542\n",
      "08/11/2020 15:29:56 - Num episode: 1050 Mean reward: 10.0200 Prob random: 0.1255, Loss: 0.0532\n",
      "08/11/2020 15:31:51 - Num episode: 1100 Mean reward: 8.5400 Prob random: 0.1005, Loss: 0.0541\n",
      "08/11/2020 15:33:49 - Num episode: 1150 Mean reward: 8.4800 Prob random: 0.0995, Loss: 0.0563\n",
      "08/11/2020 15:35:46 - Num episode: 1200 Mean reward: 8.2200 Prob random: 0.0995, Loss: 0.0586\n",
      "08/11/2020 15:37:44 - Num episode: 1250 Mean reward: 10.7200 Prob random: 0.0995, Loss: 0.0597\n",
      "08/11/2020 15:39:40 - Num episode: 1300 Mean reward: 13.2800 Prob random: 0.0995, Loss: 0.0629\n",
      "08/11/2020 15:41:38 - Num episode: 1350 Mean reward: 7.2000 Prob random: 0.0995, Loss: 0.0631\n",
      "08/11/2020 15:43:36 - Num episode: 1400 Mean reward: 6.9800 Prob random: 0.0995, Loss: 0.0640\n",
      "08/11/2020 15:45:33 - Num episode: 1450 Mean reward: 10.6000 Prob random: 0.0995, Loss: 0.0627\n",
      "08/11/2020 15:47:24 - Num episode: 1500 Mean reward: 10.3400 Prob random: 0.0995, Loss: 0.0628\n",
      "08/11/2020 15:49:23 - Num episode: 1550 Mean reward: 12.4000 Prob random: 0.0995, Loss: 0.0603\n",
      "08/11/2020 15:51:18 - Num episode: 1600 Mean reward: 11.8000 Prob random: 0.0995, Loss: 0.0587\n",
      "08/11/2020 15:53:14 - Num episode: 1650 Mean reward: 11.9600 Prob random: 0.0995, Loss: 0.0559\n",
      "08/11/2020 15:55:12 - Num episode: 1700 Mean reward: 10.4800 Prob random: 0.0995, Loss: 0.0649\n",
      "08/11/2020 15:57:09 - Num episode: 1750 Mean reward: 12.4000 Prob random: 0.0995, Loss: 0.0660\n",
      "08/11/2020 15:58:01 - Num episode: 1771 Mean reward: 15.0200 Prob random: 0.0995, Loss: 0.0665\n",
      "Training complete because we reached goal rewards.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADD0lEQVR4nO3dMaoCURAFUZ+4/y33j8WBibQL/jmhJqNSNJjcMzMPoOe5/QDANXFClDghSpwQJU6Iet28769c+L5z9aLLCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiLqbAFxzzuUqGnzFTG/t0uWEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEZScAH5uLbNYHV/RG+Ha5nBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKjuBODmDN8/3qKbxe/9LH7vxZ/c5YQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6K6+5ybNrdBl5ciz+aHt8n6xuWEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEmQDMWd0fJMTlhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRGUnAGdm+xFglcsJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IupsAPD95CuCDywlR4oQocUKUOCFKnBAlToj6AwOoE9qRcQbxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Once your model learn how to play and reach the expected goal -> Play a game exploiting the main q network trained with deep q learning and display video of this game to check how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"432\" height=\"288\" controls autoplay>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAJDBtZGF0AAACrQYF//+p\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2MCByMzAxMSBjZGU5YTkzIC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMCAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTkg\n",
       "bG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRl\n",
       "cmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJf\n",
       "cHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9\n",
       "MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTIgc2NlbmVjdXQ9NDAgaW50cmFfcmVm\n",
       "cmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42\n",
       "MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAASs\n",
       "ZYiEABX//vfJ78Cm61tbtb+Tz0j8LLc+wio/blsTtOoAAAMAABnblS+MhuoLgPtAADwgAGhPcIWt\n",
       "rFVUU1M3UJ56NhWTFrFZsXWsEDrScHrBt3+a5S4T/C00czH00LPfYqVfQgTph2o5WZ+Mn1p35C1+\n",
       "jeOp8FNe7eqIxhPIR6dvXlQRff46C0OfKmPp+if4EJlCk1EvVrO6CyrHs7B99GRvZV/8q8XEfkxg\n",
       "+vXr1Atjiil0hrrBqLg8mYouf123CJcKth9f5HRF/T0/d2gs0yYKdxWKRAMJPN6OyrRR7DV/e7aj\n",
       "FMe9iQVxLc2zA0TvNabCEj8zL/oDaddJMhxb9zt+A5sZXhoBznA9vWw8+UEONFITmAARj+WmuUoJ\n",
       "PDhWMKi44WnHJT4XnuOGB+SrxMB9uCxEwklxSF4Cu/tS3Hh2meU1kxLejwFtqIxdBUrVCBOkpM/Q\n",
       "J1RncEsQ8O+QhQ3H78cEtd90HHKMe49MoXm2ysTthxpuf5O9i8dimliLETr9eV57d59glj7DqVyo\n",
       "yu2AffDYI+r045S+ogaP1/gye6R+KextCt6gg83Xb5Ddn+a4G7SPk9UHtHoSX7aZF23KnsaM1UPy\n",
       "7MoPxy3ctl79KOwVeGWDGvZKnHWBIf2XuyVj0hldKuTCFhHlhwV9bilGWtJSrw/Xj3DMq5O/G+4q\n",
       "zvPCnQrlJFmjtyPMSbEZ1KULmR3v3TJI1U8akvhafFNNdW4oVvpic/TxjrNwyV/F2wP9/KyYDBKG\n",
       "5AuUpzLv3UP3sxoEVFAvskh4sZHMaPiQSooJSvoq4pC96lm8fWFvp+ICrQhqKj6C41/kmdUjYsjT\n",
       "He0DmDgWcP3DEaORBm6xUnoROsy71i+S/S9DcPAg6Q8+U5nIqflpd8+fPScaayByvQj8Gmwkfc5/\n",
       "nyHiPlKI+IBypNLI0zbfoWMfvk2l2793Twqa/N7gyH5qrpyNYPIdtXWZJJKYYkXCiyrRnx+dZByN\n",
       "PR8iQEbhFjej5RKx5rRPshLZXXZ9damAKcynGFqPbumUKpeby983IH3U1OofeDAnrTt+VeRrGN9K\n",
       "zsk36LzfaQa2Tu6EVVDI9VhNPcqnaftVxuYVtuCfhSI4ms0SP+NedMXH+1xAFdfYUnyVHpaYg3Yq\n",
       "4zRMYA//6UYW0EKSRXveTm/3nGTjjtGpLjuEBapxpbm8OnsXZRJxJJUtXpKo0MDHz6SZ34ooNlOc\n",
       "Z+PcCOSd7pt8eqW/zMtbAI+UddFLaO0XhdreAmOtm4DxulZ0LDAkG46HCsTrI8OlOLCaGNgBTnA9\n",
       "WmUXzJ/SJamV87BBlQLnlfn6tX0J8Co63Pmo9/1IbPfD75dRYbfVffFbtsrKQqYKCppxA/slXJ0w\n",
       "QmPv7UldHquUUj7/7cD/8u9M4/SOuKeMfIo/hosWnTuDFOHIlQkh4v6UlSAGNUX7nUbooA0Us6gy\n",
       "mgR2W5M60ALsBucdqjP1p7ubhazDOKYN2zmqQR3DZ7Oid17EeWgle65StrSIV7wcK3Uk+n33Kh7h\n",
       "qw/D/V6mUEReigWxttw+yO6rZWNAe4mXfZzy8vwd7E2Q02ch/0aJGXPl4YBgC6xb5IgEaQgAQEEA\n",
       "AAHAQZohbEFf/talUACuC/b3F2fz7+jxgftG+AHCCOUWh2leRz9bWtQVCP23Qm7bZz9qnyFnH/XZ\n",
       "vgYxU6EdLW/Axz3upzOeQTNz7TayCvPU83+the9w+1hYaQ/KBGFcOkn+4sAd0J/Djcx8+su6psPL\n",
       "BFY4dRw3739AP0DlSCc769mDYTT2Vfblwg5XnFSBnn2Wa7dJ0u5Je2BPnjsKih3UmJY7LsACrVnQ\n",
       "IqfXylsukwjjYjEpiGkIjUqzOxmQayl8373UuP+uxc5DQw2Q0OETDpJNFwPbVslMwgy/UDIK1vM7\n",
       "4J66pzPTg3cXhVVChcsehH1v4OdHJ6oh0xdNX00eqowIiE0Rz+AzJCJEb3IptYEu8hbn3gv1g+3F\n",
       "B10l2b2D62S8vzJcps0mhE0ObWaOU+REtuwXHEJJcZ3+LaOMZ8CHlObQfmYExpNnpP6fjGY/77Tg\n",
       "MErkuL/YksUm7OftPaOPe+CCffNi+imQDcjKoYs3UFbcU7fMzdF3Aq0J3of+AOdGxKf8ayH29IQD\n",
       "BV1RkDR/kVyRpZwZVXBLTcwH9cqS7kThidXneXjzIRjS7oKNETG4U7VdVbsafpJeWWEQQAAAA0FB\n",
       "mkM8IZMphBX//talUAFm+tPSW3O5Enwal4AYjZ4rhgjLMtHM1Wbta3AglQEYd/d+/+/207VayvjN\n",
       "L5nc5gQo0EoWlV7FpoHkuMfqh2zSn6TMBX4a5xHdhFDV9edBpMOYcn1+Kr/CAWNK0LakNpUIauM1\n",
       "9ebBWNaoyCWRPUQJKlY6VF/cyh82aJFUNdpPnZ///66HLBWtF5s7X+CGryfq8ahX44gLzyY1jXf/\n",
       "IqWorOk1tMQGFzl6OSIAyDppwquEbvvYc0VgAk+DNZSgHZz56/q4fpE3HrT2nHvtjbqinrZLD450\n",
       "GqzGNhSTUKEVeL67z76Pc0H+ilkjL4jEJHhwNMBdAH5ze++K4iuk9Zozi4BhcBXEIsth0JTE1nQ/\n",
       "9zqw7mwpULQgqMJoPsBRabEsSqezTKGM9ttmmYKC7Ac5w6WBBcO2irO4kyUqg4NTi6ar0e9fhhZK\n",
       "MSVXiPhYqfw5k4JvUB96gcQ9ntRbsBCL8TPF2a7pH+JQDaM2/hETneaJBji16A+3PF0LJ0D/bZfz\n",
       "39NTpuqhIP8t3vRb9vH1gQrhEhbAS4vI7brwqKWX2Qu9XZuswX9dvvjBA3B3PfGyvqoUO4tM9lJ5\n",
       "6MQlgOveqyNqQZloOKbLQgszpZsxfbclxYA8TT6iX78VszxlvBLuc5u8Qg+ycu1/ftqjqKhCrPW9\n",
       "hci5S+m1pJEsAfm2/+3atcF+cBOSRVqMgLonqMbOkBPb+FN88vwUMf7oklHLlaMylkCVTINbCdhU\n",
       "0Yn1nPWlK1BQwdKFE49IwDxhyF1It/k3xo1ZQLCskImoylTh8lu9QQkDUCnEFk3uob6rqyiL43RE\n",
       "9RJxTf5R+vSK9B1H6DAU7xo/BP8OVgbfUd0LgRX16KNU+lD+6gaKEfti2cbJ025mbu+qp+hxSyxT\n",
       "OTx2//b8Ufaa35piZ4k/PjmVc873RBJFVyK7xD5XqQ49FV8uZo0M/Gqmut+3z8A/NSVCfBbIcnOu\n",
       "5dvCJW07MfXf/Zein8n46pTQkF+AJX2fERamEvupYEpqsBmpUYCCDX3jfdybtHxCic/dLizON3le\n",
       "NaiQHWa9MMiq5a+uPhUE+w1KugKeiNHFyvBsQda5f031awAAANQBnmJqQS8ABWRCb+z6b2PKd3Kl\n",
       "NFDE1JvpmFqN/UAnVc7rQHFwFjA78dkXiYSUXsOTZ0RXoXj6X3EVoW//e6NatHOcEQYNsZoiFcMp\n",
       "/Tvi+UupPW37gML9y5ILlluMEcV/Wf/OOajxUSoPeZ6ZU+MMnePc6o4IFDdACwy5DxxAN5d1X84P\n",
       "ZhVYTPnKvX0oZ3Oke2cgcxtf8aRvRUzDS+PF7J5cskxhaKHDBWIZikWxkndap3m7B/ZfpiS8QALL\n",
       "woyYOBA1mwrf7aB7TF86LrPOnnXymAAAARpBmmVJ4Q8mUwU8Ff/+1qVQAVT609JbO1OEwJYDhBV9\n",
       "//qALgml6F7QDcKecV2q36u/JGga1uhGB++LHYCFtL0On0bEfqXoXyPMPr4BSujOLtng2FGSpW9h\n",
       "ODuQrW89C4G67FS0YJOGwon2L1X3XxWO+/t6PkT9hfGBywsCGK1Y6sEnmDHPmVTfjG9UalBebIA9\n",
       "EwGRWmwmWB/9j0m4thxnEWWMZkmlQ/3fE3zfoPNBLjFFV8XOaHiP3xcMyHVMEwQHkSns77FNA9+v\n",
       "Sgf9QdR9J2zsB+tRSPMEUBHfl2O3UaFX+kpgOtrx7+tCoUXrVXSCQrJ9u9djlLrx1uE/dkkTz787\n",
       "X1bVYcZygUgdP6kR1gI1gsJNl6OqUeEAAACRAZ6EakEvAAT4DV8FbUHcBJM1oTnuwA2fh3TZGrk/\n",
       "X/i42RVyVKz0GyVfmZ28y4pfk1nCsm9vJrfrjoWcMP3x7nbWrIL/ENHBhuxv34qGO1M4WTG9xWVQ\n",
       "CJjtHbgMI2N3aNL+EugqvecwB43som3jdSnkXv7DU3qdekgsgX1NqSqV7rk06l12ocbNDxNMvkhe\n",
       "gQAAAfRBmodJ4Q8mUwU8Ff/+1qVQAKCZ1RAF6Pw7WZsjadc797A0M3UzO/JeC6NURR34D7nca09P\n",
       "sbJZxX9hxEiq0nCzOiP9M2DHGr6YZJ2x44v/oky7/9/kvwxqAFctXzWluZbZOP0RccUbr/8RcQI6\n",
       "bMoDG9pG5unywKINR/rptdKnz/bz3oty0p752k+BUnkTEMsjAMVroCm04Xk4sDMwISpI6UL2h5ev\n",
       "FlrtOFHmJ8O6PU+zF4E+upOcGvCLhmet0wCCcJEq528aHag/5MvsdttPkjh1P1rWvFGWxeU358Uq\n",
       "fGzfkOtVpcNLNF8FbZqd3FqfBeeSzrCqidm82SCcgSlEgRl9odyUbQO3FXBsFbymMVE3asz3pKT1\n",
       "O5l//WulGBftW1NaUEhX8BLGsiX6duuQoZhBmR4wqIp9cXnVQQuhXRmPfh9+1F9fP2o9tbALUUWi\n",
       "C919L4uzLJuUxyhIW7aucGJ4GqDpyEbM11/lfN1f1MY8lZiM9VYQzzu4/do7Tw+BKTySwWPQEpPz\n",
       "pM1CVwCxxQEkKS0ZZ3L3vCu1M4YjoWxt/OaAPbHANjmTO5O71eVK76WDJ0ni7cdHSHyA+5McYJnf\n",
       "nmcBMU0zwoUZhTXom+RYUhFRahANViqmDw4dipTOz9RP1YYIbdQZSDcqTBGZmJODgQAAANkBnqZq\n",
       "QS8AAlo+sht22/rgbX23XPbuGPaJKPMZF5CIDWF+/oQATWcSf+QxcRHMUZjWfZt1TKfpD4L06jVM\n",
       "IEo5VgHUSO0MphNZ2h1LaKcyP96+WlhroZ1PCQJHPnx/4sR90fxwvcFrRQt357OTSnuSV3baacin\n",
       "aOrh83iUt5WQL/Csy/VdVQMRoN15cY3c5aDWQzjG79YVLNSbUAPP+x5aotnEumpaBBN7ORpcL+QZ\n",
       "Ttgx2pgYrM/5NdyzoOivjeQlKFeFmIlP3N7Xe4zFOZBnIv7m/Cqwar23AAABN0GaqEnhDyZTAgr/\n",
       "/talUACc/efyMPXko+LcherowAJ3ObPTOO1PUA2OS0Kz74EQ8CRnVbBrC0l/cmg//AODxT+chG/a\n",
       "0u4zYuqdgbIspk4wGEw/ygMtZlRJz2X3lZnU+gtrEMmaY6sVkP7uo2ASs3xJyms4bUWr7dqmrfwP\n",
       "rvVPXo8EX2mQ2fgg/qQPvKIbSlQqeY639kSrBMrLS2m83hndaOVBjoNCr8tpBZC0rfFaLePfcJkw\n",
       "yVKVoVyacP0W+AwnRl39unwH+pdYDjApQdk9y8a8vAxZZeMFZzWh/ePkub5401v2slMsUJBQo1z0\n",
       "q21BjSLV+c2RLN50RgjtP3fx8/FFZwj6TFoXo2A+BAopzQFqD5mdGDgu2us4286STEnTFSstLyjP\n",
       "El4w0jfWz2eZkeFj9y18AAACL0Gay0nhDyZTAgr//talUABZQ7MqBUqvHBL/CZ8C9XbC4HU2z5Mw\n",
       "HyPrOLF5SHeHQ+QLKBzMErrWEfU5vB4thgSk4T+cLevaSx+A/8OYrxTYG2Ue+yhO1rKpV/nI90xE\n",
       "X5ltfBPzZfrT78tsFvAgHBkFQm/qQ4uI/BEhEGXTr38ZIEmQSxDBTSS/ZM3wh7DBWmpifQd7jU8u\n",
       "MxOJPvwOGyuE2ROj8aDa7Ftz+6Og0ynB+ofrL37RlHi/mZQNtKy12EUMhR7pOoYXei0Nk5Pu/Lxq\n",
       "/zyD98wjkVLFYq2F62GjtMa+6CCNaSb2i67B4GOhmt7nCj27OR/+AitcwMbJSYKE96J76PFEdtm8\n",
       "++kD0nPIb+1pFBub8O4NRei8UJNRfpgCxf8Rhw5PcGtyr+2l1OqpxVIrK9OoljiH4vlS5Ak5Mnc5\n",
       "Fo/6qLK7cALjAqgX6xRhmDG9WKDNSRr75WwIYZgvzlK0D1joB2OXFfR4CBZwQ8mV/cmr5+aawLfl\n",
       "Z4wWxc1D+QETvvWk01nK5Ost3KX9Zcxf2IKwhb5UZWjkxBMJQ2rc18r9CFHs1OLQDm6HVLnZuV+s\n",
       "YPcdPspohNBR2nCh+ryyWRcyjPE53GhFflND0kT3HWyPdX7NCxu7KjcIugU12m4uFSzoH0aA0ocX\n",
       "Vwu5st6LYQrTun9bRCqD8MmzMz2e8s6d4+017aIUsWIhBptt7NjLUVkHjv4jMilXhSEW1z40oA0L\n",
       "gcZA4QJCB/wAAABWQZ7pRRE8Ev8AAVjf6RkFiDOthq0UYZE0AEL/xzPeqHfqJpRaN/0IPkCsQ1sS\n",
       "VyjO7Z1DlbMrVaNwoYVMsk2x50VjudnmVzre8bqC/BoShlnJVyezzuEAAAC/AZ8KakEvAAFY3+kZ\n",
       "BLgAil734F8mQIruf38EImjtpe+7aW5SMorS8Hyfl4/4D/w5bPE9fhq/MsGB1di6ZGwQp/1T5AyN\n",
       "6pQQiLmhYGqLdm5E4TZTPKlrmQAHfKzry7RRyug5qMIvhvikbHxdy55GuaQOSM3zJ93LxZsIbS4l\n",
       "lbzn3ZiMp2MpchPEOfpm0JyQtdBmP1eGw8J08/mjITO2MHymo+iAzkvVBm4jYoakyTYakDrIA/HK\n",
       "2GylkwNwIFcAAAOEQZsNSahBaJlMFPBX//7WpVAAZa85/4uV1vwJ+ZHNt982UB2lVlgIINiW7+Dw\n",
       "0ShemE/4t7hkxePbhJ0w0javhljQrq8Xq23B3/DV1i+zpDCs3TRhuFfGFTVJPMffEUM8okGSrH5I\n",
       "1Tmx/NoBZzrNdU8vD8wBVNYqGK7BS2kwn3WwM8f7eSQvRu7tRdb0S0cxKtoCZX7CPxg1a7NboBUO\n",
       "q+o3tWo2pq5YFe4ZXpG9DVgf+6zmOsPnWnVlRYdmjDk114QyJCg/fZxcxMu0P/nkz5RtJ6E9zAVl\n",
       "Wm6SDAdxe0Y7G56bKcaK5Vjsd5wflySWNUeqed1PYSvSBfW+n78NSIZEBtYRdyKwHTbmgR066T4B\n",
       "Nqfx1pkAinz4JqQ9cki4pQObgsePrH7IDGs7fI30gk3ERN4dIkW8DETmbN1GPyRnmBdwiGUxPeN5\n",
       "lErFPLI5xTdBOraRh2oDv5tWHhQ6T+/IcmwBb65Ax2lZmigQvWmW2vAW0FeEC6CsVHRYpX6GDauv\n",
       "dxTEErAcdjH7iHjgQWjzT1uHX+ENXb4Ocs38S7jdgJVKHCLqR0laOXxI/8ctwZH9kN94T1VYbZE6\n",
       "yDCeuVQ/Ygp/38+7c4Oqp98lnnv7e5Kco8gltkjcdB+98PbhpSeQdsi7wfIOaKYWFzxZOgGPeM7U\n",
       "hYE3Uy//rTrIj0FfiB0BSivcACMDECxTSZlyuU/STXlzdMtOSMlJ9r+pxZn1ajWxdU3ynz1u6r1y\n",
       "IWFFpylLvnDgQuMsX7b1YweCcOJKid8PAPsGVQmXJDA+CICvNasKvAG/WvU2z7UWGSlqqcu0Uibx\n",
       "rgKGJCqgs10b6B7dXwt/HlYragYaQ1og+yKAvyfstEFD3m5fk6XFXOBdsqC+S0orexkg3wgEY+Cd\n",
       "MRXjFry+BZkbeUUK4UJjJ1aII2Se0zBNcNrXa/p+Mv4RcTfI2h/+44JG303MbvW0cr6LDn01XztA\n",
       "6ZvbFZcVxwxByrYx6SvHXOJTwRlifMyqsTGKDJnxfBbMK3VQPciixPO3mAFrUZYEj05Okhgdj0yT\n",
       "3OhpGVSS4wdw+jZuYk4I6NKY4dDlwjlwYuJ4WS7kqMCxV0wWtru6m6h0472TQOKdNdekzsiyFUU1\n",
       "fdvf9XrP0mV1iJjQtbV5RM0VeLbCF72eXjCjRNDaWSxg7YuYLUVWa0wWE5tb+I2rziCKVy2HiLRf\n",
       "AAAA7wGfLGpBLwABh3MoCKsqnS2hH/LRuH2DfllnaOLpowRRkJKz052UOLO0suuAEVRtaUpeMl2X\n",
       "kyFAuk3dtJbYr7Ce9LDvBmxbzazBc+ZnfD+GvhjrCBowp2bZ4BpPW7KEzSzQL2UJeSMIykXmGk63\n",
       "wSB2S0AmRh0QrXa/5sfH3fxB7F6JD/YE27pI7Q7XDz1BaahPpyqDT+vOBSsj91N01J52+rtcvVwy\n",
       "7gVzz0Mo3+KkH75D5iB7B6roLb33dKXZbqxRPD8rb3uYLQIsPLewqSkyiyzQpPrr7PCjneR1zqOS\n",
       "jJhwbuKTE2dtmrKvJ8i5AAABTEGbLknhClJlMCC3//7WpVAAYrdtXYvPqhOAA0DOXn186RYVypHx\n",
       "x7Mx/+V1UoDsoDY/5Q7efSS4sMf+kiFV/aH0BvXNsN6tB5mX3FGYouQLs1vQOG4+1tYjgJs/6SOG\n",
       "bP2h/M3i5nFufaj1ivLVtdcza7tnlJsVAT6d/Tq1gZHZzXrHMGs8a1ViIKR8Zu0ZLUq9C0SY66u5\n",
       "XWYtFgbSPDEqtSFzOaXnWkqVv5dv+mJ/m3LixK5s1jV708P9UxqKLGJgADRxQC7f8GbQPaLRftWP\n",
       "+Af67YW4DURL3Jz1gf+aRsKg+kTgjji2WpM09bvFb2dU2OsgFUdMNfcJ6GqAevaFRu3e3r6uvop8\n",
       "6xpsbeVzlGvMewRJ1maGbW1Bt8YZOM9dKzDYJ1FtqCWuBfm8NeDsIgYRN4y3Tuw8/5BR8MlLluAe\n",
       "Akxpg4cjW2uhAAABzEGbUEnhDomUwU0TBb/+1qVQALfrPH/4451Is/fPZ96WP02Wkkh6NgA6V9WE\n",
       "GA3l2hTCAQHaLQ41sQXOllp39+GaFDELp9oA7BfWkdclRraYZzLMhtd1n4MZ90rcS374eDu0jQsJ\n",
       "G/103xwyLOefn5527VPUyS20wPl5H5ZMG2DZdfZtwAaEY0bqSTvhvjKVd7wfJpLXA0fGhN856O0n\n",
       "L8KDCFJdppj4eV2iWMVUX90rwqAWhIbdqEcfJcOV+gsboz0ajmoWWD0OjpD9RKrkmYNwMbeW+shc\n",
       "NaKPE/q/pDgGBAYETdIucYZbLqF/c7gXUpgUoiIxUAGz2cxXC0zjIqIsyc7E7X9YSFTWSYoG6tCd\n",
       "NjkfldvAeyMU1SvQ447KbNcEA5/MMQzKlXXusMWgkyBtCplypLPxgkBFzlbuv3D6gmyQmXa1hu/q\n",
       "mLkoRKauGoq3/MOnAjVBrY9U4uefTPuyUwwFSX1UAJeAK4Ly1omFoagAFcGxyWjmsYXtklCxKzA0\n",
       "N6v9M4Da4rijeND4eYFGpCivZxRYUZZJZiLa1cpiT4iKYz/7efmEGhyLchKnFSfq+g737tMEsxsB\n",
       "PbSE4vmHWVjLhkIOj5VMu1EAAACoAZ9vakEvAABabLH5uA8lxduhovlixSb4ECwJA8M2IlpAeGpe\n",
       "OhgBnBaEKzK47EVfL339ymsnpda3tXZ5bwI1FmhM8oEaDBf1zC+uhIk4orzXlWkG17ZGyktoXcPT\n",
       "Ku1B5twVSDSB0QUf/5BGNwNJKa7av5gyqHp4sRWZianfl2g+h4WQCNxbusP4rGPR1ffnr2Nn40X4\n",
       "ujYpuvZPpJs8nVIN6PcgKlhAAAACZEGbdEnhDyZTAgt//talUAGA42XE4wjL8GLBJVaZPFFLnc2T\n",
       "SM/KWesf/jihtnTSF1+Mnurb2d/bCljwI/wTSEWqTRzr3IiqQ6GXeNGFv3+UA9tVH2kEThHaaAFo\n",
       "HfwCMocC9d8ime2ZH03KMXpZmt+z/tOupBqS7dEtl4L6A8RPmiQJl8MoC0zmIx2NkqdCLsbks4Sq\n",
       "g6b/vf4KTbupHiTij9k13kPQYigYNNwehGfUGsg4xfUcf8ttn+w6ul/c55nBN/KsT26NsW9z8jUA\n",
       "B/On6WStGRgHphVIXd7RTRtmARFmpO+eVyIT3LOstyccm6iya4ahfujG74Mgvlfo5RYh327P3Aye\n",
       "ZE9AHDCaHyR6GZq3KWHZWfeVgF89IAAPyzBj20HC5zb6+kJjw42R/CWGqCsbBfqkC+JXpJY0NtJv\n",
       "FSlGyN99I92rYVXnE3XNESQCLIkjvGfRlEHusLoqbxsg8R3mfuiZf7xMEEU8UWIZvt/BkEZxumJb\n",
       "/VeFuWt5F6Eb9oUvPXLDk0WBdInv2IevTbeLQmg/NzGc4E06SxTbed1NaQxMqUEmrHDRo/GwcxJ2\n",
       "e5xaPnBl2H5wTGdsltIyNC/VQHwhU3rf//B/XV1dCyhnl19K1D151V/c7yxXAtCKuy2JjLacxDm2\n",
       "3/FCTh40Si7hsq8drbX3TPeGGvbp/2CPtrPurVDecYoExRhJ+d7HDI/Y22JkBv/9Sa8BbgSuRCNI\n",
       "11JEEbu0tLdcIro3QANAI8ZDcVKQvh9WQmIVomrUh8JvM7QmYmkz1PBY8pY1C8LWXZY5fWZxHxfL\n",
       "AD2Mu4h7fgAAADhBn5JFETwT/wAB8wjUrYuy5i8TwZaZlBetEhcdgc2/IRcjl7jgEmU1KJRL2iiz\n",
       "GoP06lurYKpswQAAAD4Bn7F0QS8AAvqxd7+fp6A/SwtVSkjowwT3Mqm8b5gL0phcTbgjLxIyZXtL\n",
       "Jp3RTJK1ZhK9bjaxbk98eSQDugAAAA4Bn7NqQS8AABLSft6N6AAAADNBm7hJqEFomUwILf/+1qVQ\n",
       "AGA4ev/UOseA2Y3PySiJsS1jkuzAPl87YX8h44SZbe7VkdUAAAAWQZ/WRREsE/8AADPB+yz9fGJ0\n",
       "vgnOwAAAABEBn/V0QS8AAE1EO2pPAP96wQAAAA4Bn/dqQS8AABLSft6N6QAAABVBm/xJqEFsmUwI\n",
       "Lf/+1qVQAAADA+4AAAATQZ4aRRUsE/8AAAyhHwKd53DttQAAAA4Bnjl0QS8AABLRDtaN6AAAAA4B\n",
       "njtqQS8AABLSft6N6QAAABVBmiBJqEFsmUwILf/+1qVQAAADA+8AAAATQZ5eRRUsE/8AAAyhHwKd\n",
       "53DttAAAAA4Bnn10QS8AABLRDtaN6AAAAA4Bnn9qQS8AABLSft6N6QAAABVBmmRJqEFsmUwILf/+\n",
       "1qVQAAADA+4AAAATQZ6CRRUsE/8AAAyhHwKd53DttQAAAA4BnqF0QS8AABLRDtaN6AAAAA4BnqNq\n",
       "QS8AABLSft6N6QAAABxBmqhJqEFsmUwIK//+1qVQAAFcMOgAAqQdgXzBAAAAE0GexkUVLBP/AAAM\n",
       "oR8Cnedw7bUAAAAOAZ7ldEEvAAAS0Q7WjekAAAAOAZ7nakEvAAAS0n7ejegAAAAbQZrsSahBbJlM\n",
       "CCv//talUAABXDDoAAIXoO1tAAAAE0GfCkUVLBP/AAAMoR8Cnedw7bUAAAAOAZ8pdEEvAAAS0Q7W\n",
       "jegAAAAOAZ8rakEvAAAS0n7ejegAAAAUQZswSahBbJlMCCf//rUqgAAAHrEAAAATQZ9ORRUsE/8A\n",
       "AAyhHwKd53DttQAAAA4Bn210QS8AABLRDtaN6QAAAA4Bn29qQS8AABLSft6N6AAAABRBm3FJqEFs\n",
       "mUwIJf/+tSqAAAAesAAABXptb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAABOIAABAAABAAAA\n",
       "AAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAAAAAAAAAAAAAAAAAACAAAEpHRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAABOIAAA\n",
       "AAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABsAAAASAA\n",
       "AAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAATiAAACAAAAEAAAAABBxtZGlhAAAAIG1kaGQAAAAA\n",
       "AAAAAAAAAAAAACgAAAMgAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVv\n",
       "SGFuZGxlcgAAAAPHbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAA\n",
       "AAABAAAADHVybCAAAAABAAADh3N0YmwAAAC3c3RzZAAAAAAAAAABAAAAp2F2YzEAAAAAAAAAAQAA\n",
       "AAAAAAAAAAAAAAAAAAABsAEgAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAAAAAAAAAAY//8AAAA1YXZjQwFkABX/4QAYZ2QAFazZQbCWhAAAAwAIAAADACg8WLZYAQAGaOvj\n",
       "yyLA/fj4AAAAABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAYc3R0cwAAAAAAAAABAAAAMgAA\n",
       "EAAAAAAUc3RzcwAAAAAAAAABAAAAAQAAAZBjdHRzAAAAAAAAADAAAAACAAAgAAAAAAEAADAAAAAA\n",
       "AQAAEAAAAAABAAAwAAAAAAEAABAAAAAAAQAAMAAAAAABAAAQAAAAAAEAACAAAAAAAQAAQAAAAAAC\n",
       "AAAQAAAAAAEAADAAAAAAAQAAEAAAAAABAAAgAAAAAAEAADAAAAAAAQAAEAAAAAABAABQAAAAAAEA\n",
       "ACAAAAAAAQAAAAAAAAABAAAQAAAAAAEAAFAAAAAAAQAAIAAAAAABAAAAAAAAAAEAABAAAAAAAQAA\n",
       "UAAAAAABAAAgAAAAAAEAAAAAAAAAAQAAEAAAAAABAABQAAAAAAEAACAAAAAAAQAAAAAAAAABAAAQ\n",
       "AAAAAAEAAFAAAAAAAQAAIAAAAAABAAAAAAAAAAEAABAAAAAAAQAAUAAAAAABAAAgAAAAAAEAAAAA\n",
       "AAAAAQAAEAAAAAABAABQAAAAAAEAACAAAAAAAQAAAAAAAAABAAAQAAAAAAEAAFAAAAAAAQAAIAAA\n",
       "AAABAAAAAAAAAAEAABAAAAAAAQAAIAAAAAAcc3RzYwAAAAAAAAABAAAAAQAAADIAAAABAAAA3HN0\n",
       "c3oAAAAAAAAAAAAAADIAAAdhAAABxAAAA0UAAADYAAABHgAAAJUAAAH4AAAA3QAAATsAAAIzAAAA\n",
       "WgAAAMMAAAOIAAAA8wAAAVAAAAHQAAAArAAAAmgAAAA8AAAAQgAAABIAAAA3AAAAGgAAABUAAAAS\n",
       "AAAAGQAAABcAAAASAAAAEgAAABkAAAAXAAAAEgAAABIAAAAZAAAAFwAAABIAAAASAAAAIAAAABcA\n",
       "AAASAAAAEgAAAB8AAAAXAAAAEgAAABIAAAAYAAAAFwAAABIAAAASAAAAGAAAABRzdGNvAAAAAAAA\n",
       "AAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAA\n",
       "AAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC40NS4xMDA=\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/play_game_with_mainq.py\n",
    "state = env.reset()\n",
    "frames = []\n",
    "num_step=0\n",
    "done=False\n",
    "while not done and num_step < dqn.max_num_step:\n",
    "    action=np.argmax(dqn.main_qn.model.predict(np.expand_dims(state, axis=0)),axis=1)[0]\n",
    "    next_state, reward, done = env.step(action)\n",
    "    frames.append(next_state)\n",
    "    state=next_state\n",
    "    num_step+=1\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D3QN\n",
    "\n",
    "DQN has  been improved recently by using two different tricks (among others):\n",
    "\n",
    "* Dueling\n",
    "* Double DQN\n",
    "\n",
    "Implement these two solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dueling\n",
    "\n",
    "See the dueling architecture : \n",
    "\n",
    "\n",
    "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n",
    "$$Q(s,a) =V(s) + A(a)$$\n",
    "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn’t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_2_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_qn = Qnetwork()\n",
    "main_qn.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double DQN\n",
    "\n",
    "\n",
    "This lead to often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn’t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n",
    "\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma Q_k(s',argmax_aQ(s',a;\\theta);\\theta_{target}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/exercise_2_7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning on Pacman!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the MsPacman environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the images is optional but greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return img.reshape(88, 80, 1)/128\n",
    "\n",
    "img = preprocess_observation(obs)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `preprocess_observation()` function is slightly different from the one in the book: instead of representing pixels as 64-bit floats from -1.0 to 1.0, it represents them as signed bytes (from -128 to 127). The benefit is that the replay memory will take up roughly 8 times less RAM (about 6.5 GB instead of 52 GB). The reduced precision has no visible impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
