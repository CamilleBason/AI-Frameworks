{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/wikistat/AI-Frameworks/blob/master/IntroductionDeepReinforcementLearning/Policy_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Introduction to Deep Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Policy Gradient Algorithm\n",
    "\n",
    "The objectives of this notebook are the following : \n",
    "\n",
    "* Implement Hard-Coded & And Neural network policy to solve the *CartPole* Game \n",
    "* Implement Policy gradient algorithm to solve the *CartPole* Game \n",
    " \n",
    "Source : [https://github.com/ageron/handson-ml](https://github.com/ageron/handson-ml) and https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%202%20-%20Policy-based%20Agents%20with%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & Data (Google Colab)\n",
    "\n",
    "If you're running this notebook on Google colab, you do not have access to the `solutions` folder you get by cloning the repository locally. \n",
    "\n",
    "The following lines will allow you to build the folders and the files you need for this TP.\n",
    "\n",
    "**WARNING 1** Do not run this line localy.\n",
    "**WARNING 2** The magic command `%load` does not work work on google colab, you will have to copy-paste the solution on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir solution\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/blob/master/IntroductionDeepReinforcementLearning/solutions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# To plot figures and animations\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "#Tensorflow/Keras utils\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Library\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions enable to build a video from a list of images. <br>\n",
    "They will be used to build video of the game you will played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# AI Gym Librairie\n",
    "<a href=\"https://gym.openai.com/\" ><img src=\"https://gym.openai.com/assets/dist/home/header/home-icon-54c30e2345.svg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple environment: the Cart-Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### Observation\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the righ&t\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between ±0.05\n",
    "\n",
    "### Episode Termination\n",
    "1. Pole Angle is more than ±12°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "The description above if part of the official description of this environemtn. Read full description [here](https://github.com/openai/gym/wiki/CartPole-v0).\n",
    "\n",
    "The following command will load the `CartPole` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reset` command initialize the environement and return the first observation which are a 1D array of size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "env.observation_space, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the four outputs above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `render` command allows to generate the environment which is here a 400X600 pixels with RGB channels. \n",
    "\n",
    "The `render` command for the `CartPole`environment also open another window that we will close directly with the `env.close`command. It can produce disturbing behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "print(\"Environemnt is a %dx%dx%d images\" %img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can then easily be displayed with matplotlib function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is composed of two actions push to the left (0), push to the right (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** : Reset the environment, and push the car to the left untill the experience is over then display the final environment. \n",
    "**Q** : Why do the environment ends? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_1.py\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    if done:\n",
    "        break\n",
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "plt.imshow(img)\n",
    "axs = plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard coded policy\n",
    "\n",
    "How can we make the poll remain upright? We will need to define a _policy_ for that. This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's start with a completly random policy and see how much time the poll will remain upright over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()           \n",
    "num_games = 100       \n",
    "reward_sum = 0        \n",
    "all_reward_sum = []   \n",
    "num_game = 0          \n",
    "while num_game < num_games:\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "        \n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a complete game\n",
    "\n",
    "Let's run one pisode with his random policy and save all images representing the environment at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and _vice versa_. Let's see if that works.\n",
    "\n",
    "**Exercise** implement this policy and play 100 games with this policy. What are the means and std deviation of the reward sum over the 100 games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_2.py\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "all_reward_sum = []\n",
    "num_games = 100\n",
    "num_game = 0\n",
    "while num_game < num_games:\n",
    "    \n",
    "    position, velocity, angle, angular_velocity = observation\n",
    "    if angle < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "        \n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Vizualize a complete game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_3.py\n",
    "frames = []\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    position, velocity, angle, angular_velocity = observation\n",
    "    if angle < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What can you say about this strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network to build a better policy. <br> \n",
    "This network will take the observations as inputs, and output the probability of the action to take for each observation. <br>\n",
    "In the case of the Cart-Pole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability `p` of the action 0 (left), and of course the probability of action 1 (right) will be `1 - p`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this problem is simple, we can define a very simple architecture for our neural network.<bR> Here it's simple MLP with 1 hidden layer and four neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the network architecture\n",
    "n_inputs = 4  # == env.observation_space.shape[0]\n",
    "n_hidden = 4  # it's a simple task, we don't need more than this\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = ki.VarianceScaling()\n",
    "\n",
    "# Build the neural network\n",
    "policy_network=km.Sequential()\n",
    "policy_network.add(kl.Dense(n_hidden, input_shape = (n_inputs,), activation = \"relu\", kernel_initializer = initializer))\n",
    "policy_network.add(kl.Dense(n_outputs, activation = \"sigmoid\", kernel_initializer = initializer))\n",
    "\n",
    "policy_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the action\n",
    "We can now easily predict the probability of both actions given the observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "p_left = policy_network.predict(np.expand_dims(obs,axis=0))\n",
    "# Choose and action according to the previously generated probability\n",
    "action = 0 if p_left>0.5 else 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** : \n",
    "\n",
    "* In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment's full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. \n",
    "\n",
    "For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment's full state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random neural network policy.\n",
    "Let's see how this neural network policy perform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "num_games = 100       \n",
    "num_game = 0\n",
    "all_reward_sum = []\n",
    "obs = env.reset()\n",
    "reward\n",
    "while num_game < num_games:\n",
    "    p_left = policy_network.predict(np.expand_dims(obs,axis=0))\n",
    "    action = 0 if p_left>0.5 else 1 \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly initialize this policy neural network and use it to play one game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    p_left = policy_network.predict(np.expand_dims(observation,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "plt.close()\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is working. But it's still acting randomly because we do not train the neural network. Let's try to make it learn better policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn a given policy\n",
    "\n",
    "In this part we will train the neural network in order that it learns the simple strategy we hard coded before : if the pole is tilting to the left, then push the cart to the left, and _vice versa_.\n",
    "\n",
    "We start by defining the neural network policy with the same architecture than before. <br>\n",
    "This time we compile this network in order to learn the wanted behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network=km.Sequential()\n",
    "policy_network.add(kl.Dense(n_hidden, input_shape = (n_inputs,), activation = \"relu\", kernel_initializer = initializer))\n",
    "policy_network.add(kl.Dense(n_outputs, activation = \"sigmoid\", kernel_initializer = initializer))\n",
    "\n",
    "# apprentissage\n",
    "learning_rate = 0.01\n",
    "adam = ko.Adam(lr = learning_rate)\n",
    "policy_network.compile(loss='binary_crossentropy',optimizer=ko.Adam(lr = learning_rate),metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn this policy we will first define 10 environment that we will play in parallel at each iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 10\n",
    "\n",
    "envs = [gym.make(\"CartPole-v0\") for _ in range(n_environments)]\n",
    "observations = np.array([env.reset() for env in envs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration we will :\n",
    "\n",
    "* Train the network over observations we have.\n",
    "    *  These observation will be the inputs, and the target will be the expected behavior.\n",
    "* Predict the action according to the neural network policy.\n",
    "\n",
    "**Question** : On the code below, looks how the actions is chosen. Why do we follow this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "for iteration in range(n_iterations):\n",
    "    target_probas = np.array([(1 if obs[2] < 0 else 0) for obs in observations]) # if angle<0 we want proba(left)=1., or else proba(left)=0.\n",
    "    policy_network.train_on_batch(observations, target_probas)  # one batch iteration\n",
    "    \n",
    "    # Generate probablity and observation on ntext step\n",
    "    p_lefts = policy_network.predict(observations)\n",
    "    actions = [0 if random.uniform(0,1)< p_left else 1 for p_left in p_lefts]\n",
    "    \n",
    "    # Compute next state\n",
    "    for env_index, env in enumerate(envs):\n",
    "        obs, reward, done, info = env.step(actions[env_index])\n",
    "        observations[env_index] = obs if not done else env.reset()\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "num_games = 100       \n",
    "num_game = 0\n",
    "all_reward_sum = []\n",
    "obs = env.reset()\n",
    "reward\n",
    "while num_game < num_games:\n",
    "    # hard-coded policy\n",
    "    p_left = policy_network.predict(np.expand_dims(obs,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    p_left = policy_network.predict(np.expand_dims(observation,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "plt.close()\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it learned the policy correctly! <br>\n",
    "\n",
    "Let's now reach our final target : The neural network has to find a better policy by its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind *Policy Gradients* its quite simple : The _Policy Gradients_ algorithm tackles this problem by first playing multiple games, then making the actions in good games slightly more likely, while actions in bad games are made slightly less likely. First we play, then we go back and think about what we did.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "* Run an episode untill it's done and save at each iteration the observation, action and reward.\n",
    "* When an episode it's done. Compute the discounted rewards for all the episode, and save it.\n",
    "* If you have done *batch_size=50* episodes train your model on this batch.\n",
    "* Stop if you have reach *num episodes* or *goal* target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameters\n",
    "\n",
    "| Variable  | Value  | Description  | \n",
    "|---|---|---|\n",
    "|Gamma   | 0.99  | The discounted rate apply for the discounted reward  |\n",
    "|batch_size  | 50   | Number of episode to run before training model on a batch of episode  |\n",
    "| Num episodes | 10.000   | Maximum number of episode to run before stopping the training  | \n",
    "| goal | 190  | Number of step to achieve on one episode to stop the training.  |\n",
    "\n",
    "Those parameters are fixed for this TP, they are common value for this kind of problem based on experiences. They are not definitive nor results or any research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this neural network we will then used the observation of the experiences as an inputs and the actions taken as an output.\n",
    "\n",
    "But how do we provide to the neural network the information the choosen actions  was good or bad?\n",
    "The problem is that most actions have delayed effects, so when you win or lose points in a game, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? <br>\n",
    "This is called the _credit assignment problem_.\n",
    "\n",
    "\n",
    "To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that come after it, usually applying a discount rate r at each step. \n",
    "This rate will the be applied to the loss function of the neural network :\n",
    "* A high discounted reward will lead to higher gradient which will increase the importance of this action\n",
    "* A low  discounted reward will lead to lower gradient which will decrease the importance of this action\n",
    " \n",
    "\n",
    "**Exercise** : Implement the discount_rewards function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# %load solutions/exercise_1_4.py\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    discounted_rewards = np.array(out[::-1])\n",
    "    return discounted_rewards\n",
    "discount_rewards([-50,0,10], 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture & Loss Function\n",
    "\n",
    "As before we will define a very simple architecture to our neural network : A MLP with only one hidden layer and 8 neurons.\n",
    "\n",
    "We have to be aware here that the neural network will have two different behaviour :\n",
    "\n",
    "* For training: the model will take two information as an input : The observations (to predict the action), and the discounted rate (also call advantages) that will be applied on the loss function.\n",
    "* For prediction : the model will take only the observations as an input to predict the action.\n",
    "\n",
    "So we have to define a neural network that can either handle one or two inputs! \n",
    "\n",
    "In keras we define it that way : we define the layers, and we create two models (for training and prediction) that will share the same layers and weight.\n",
    "\n",
    "We will also implement the loss function, which is weighted binary cross entropy, where the weight are the discounted rated computed from the rewards\n",
    "\n",
    "Here is how we implement it : (Make sure you understand it!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_predict_network(hidden_layer_neurons, dimen, lr = 1e-2, initializer = ki.VarianceScaling()):\n",
    "    num_actions = 1\n",
    "    inp = kl.Input(shape=dimen,name=\"input_x\")\n",
    "    adv = kl.Input(shape=[1], name=\"advantages\")\n",
    "    x = kl.Dense(hidden_layer_neurons,  activation=\"relu\",\n",
    "                     use_bias=False,\n",
    "                     kernel_initializer=initializer,\n",
    "                     name=\"dense_1\")(inp)\n",
    "    out = kl.Dense(num_actions,\n",
    "                       activation=\"sigmoid\",\n",
    "                       kernel_initializer=initializer,\n",
    "                       use_bias=False,\n",
    "                       name=\"out\")(x)\n",
    "\n",
    "    model_train = km.Model(inputs=[inp, adv], outputs=out)\n",
    "    model_predict = km.Model(inputs=[inp], outputs=out)\n",
    "\n",
    "    def my_custom_loss(y_true, y_pred):\n",
    "        log_lik = - (y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(log_lik * adv, keepdims=True)\n",
    "\n",
    "    model_train.compile(loss=my_custom_loss, optimizer=ko.Adam(lr))\n",
    "    return model_train, model_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is very similar than the code use before to compute how our model perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See our trained bot in action\n",
    "def score_model(model, num_tests, dimen, render=False):\n",
    "    scores = []\n",
    "    for num_test in range(num_tests):\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state = np.reshape(observation, [1, dimen])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = 0 if predict>0.5 else 1\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "    env.close()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PG class\n",
    "\n",
    "Let's know write our algorithm without a `PG` class.\n",
    "\n",
    "Firs let's have a look at the `train` function which can be described with the following psuedo-code:\n",
    "\n",
    "* Initialize all variables and envrionment\n",
    "* While number of episode max is not reached or if our ***goal*** is not achieve:\n",
    "    * Run one step within the environment\n",
    "    * If we ended an episode:\n",
    "        * If we have reached ***batch_size*** episode generated: \n",
    "            * Train the episode\n",
    "        * If we have reached the ***print_every*** number of episode generated: \n",
    "            * print score and see if we have achieves our ***goal*** on the last ***n_test*** episode\n",
    "        * Increase number of episodes\n",
    "        \n",
    "**Exercise**  Complete the class below to in order to train the neural network with PG policy.\n",
    "\n",
    "1. Pay attention to all the parameters define in the init and make sure you understand it.\n",
    "2. Look at the train function to understand how the \"pseudo-code\" should workK\n",
    "3. Go inside *run_one_step_epsiode* and the *run_one_batch_train* functions and complete the code where you'll find a TODO mark in order to make the function train work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG:\n",
    "\n",
    "    def __init__(self, gamma = .99, batch_size = 50, num_episodes = 10000, goal = 190, n_test = 10, print_every = 100):\n",
    "\n",
    "        self.gamma = gamma      # -> Discounted reward\n",
    "        self.batch_size = batch_size  # -> Size of episode before training on a batch\n",
    "\n",
    "        # Stop factor\n",
    "        self.num_episodes = num_episodes # Max number of iterations\n",
    "        self.goal = goal                 # Stop if our network achieve this goal over *n_test*\n",
    "        self.n_test = n_test             # Number of test to run to evaluate if our goal si achieve.\n",
    "\n",
    "        self.print_every = print_every  #Number of episode before trying if our model perform well.\n",
    "\n",
    "    def run_one_step_episode(self, observation):\n",
    "        # Generate state and action for the current iteration\n",
    "        TODO\n",
    "        \n",
    "        # Append the observations and outputs for learning to the states and actions placeholder\n",
    "        TODO\n",
    "        \n",
    "        # Determine the new outcome of the action generated\n",
    "        TODO\n",
    "        \n",
    "        # Append the rewards for learning to the rewards place older\n",
    "        TODO\n",
    "\n",
    "        # If the episode if Over\n",
    "        if done:\n",
    "            # Computed the discounted rewards for this episode\n",
    "            TODO\n",
    "\n",
    "            # Append the discounted rewards for learning and reset the rewards placeholder for the ne episode\n",
    "            TODO\n",
    "        return observation, reward, done\n",
    "    \n",
    "\n",
    "    def run_one_batch_train(self):\n",
    "        # Normalize the discounted rewards\n",
    "        self.discounted_rewards -= self.discounted_rewards.mean()\n",
    "        self.discounted_rewards /= self.discounted_rewards.std()\n",
    "        self.discounted_rewards = self.discounted_rewards.squeeze()\n",
    "        \n",
    "        # Train the \"model_train\" models on the batch of episodes generated and save loss to the losses placehoder\n",
    "        # Use the \"train_on_batch\" method from keros to train the model on a single batch\n",
    "        TODO\n",
    "        \n",
    "        # Do not forget to clear place holder variable for next batch train\n",
    "        TODO\n",
    "\n",
    "    def print_status(self, num_episode, reward_sum, score):\n",
    "        # Print status\n",
    "        print(\n",
    "            \"Average reward for training episode {}: {:0.2f} Test Score of {:d} episode: {:0.2f} Loss: {:0.6f} \".format(\n",
    "                (num_episode + 1), reward_sum / self.print_every, self.n_test,\n",
    "                score,\n",
    "                self.losses[-1]))\n",
    "        return score\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Setting up our environment\n",
    "        observation = env.reset()\n",
    "        self.dimen = env.reset().shape[0]\n",
    "\n",
    "\n",
    "        # Placeholders for our observations, outputs and rewards\n",
    "        self.states = np.empty(0).reshape(0, self.dimen)\n",
    "        self.actions = np.empty(0).reshape(0, 1)\n",
    "        self.rewards = np.empty(0).reshape(0, 1)\n",
    "        self.discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "        self.losses = []\n",
    "\n",
    "\n",
    "        num_episode = 0\n",
    "        reward_sum = 0 # # For display info\n",
    "\n",
    "        self.model_train, self.model_predict = generate_train_predict_network(hidden_layer_neurons=9, dimen=(self.dimen,))\n",
    "\n",
    "        while num_episode < self.num_episodes:\n",
    "            observation, reward, done = self.run_one_step_episode(observation)\n",
    "            reward_sum += reward\n",
    "\n",
    "            # If the episode if Over and ifwe have reach 50=batch_size episodes run training for the build batch\n",
    "            if done:\n",
    "                if (num_episode + 1) % self.batch_size == 0:\n",
    "                    self.run_one_batch_train()\n",
    "\n",
    "                # Print results periodically\n",
    "                if (num_episode + 1) % self.print_every == 0:\n",
    "                    score = score_model(self.model_predict, self.n_test, self.dimen)\n",
    "                    self.print_status(num_episode, reward_sum, score)\n",
    "                    reward_sum = 0\n",
    "                    if score >= self.goal:\n",
    "                        print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                        break\n",
    "\n",
    "                num_episode += 1\n",
    "                observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PG()\n",
    "pg.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "* Use the model to play 100 games and check how it performs compare to previous policy tested\n",
    "* Register a video of a game and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
