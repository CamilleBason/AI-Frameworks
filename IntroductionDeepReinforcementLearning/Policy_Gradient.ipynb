{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wikistat/AI-Frameworks/blob/master/IntroductionDeepReinforcementLearning/Policy_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Introduction to Deep Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Policy Gradient Algorithm\n",
    "\n",
    "The objectives of this notebook are the following : \n",
    "\n",
    "* Implement Hard-Coded And Neural network policy to solve the *CartPole* Game \n",
    "* Implement Policy gradient algorithm to solve the *CartPole* Game \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & Data (Google Colab)\n",
    "\n",
    "If you're running this notebook on Google colab, you do not have access to the `solutions` folder you get by cloning the repository locally. \n",
    "\n",
    "The following lines will allow you to build the folders and the files you need for this TP.\n",
    "\n",
    "**WARNING 1** Do not run this line localy. <br>\n",
    "**WARNING 2** The magic command `%load` does not work work on google colab, you will have to copy-paste the solution on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir solution\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/pg_simple_policy.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/pg_neural_network_policy.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/pg_learn_given_policy.py\n",
    "! wget -P solution https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/solutions/PG_class.py\n",
    "! wget -p . https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/discounted_rewards.py\n",
    "! wget -p . https://github.com/wikistat/AI-Frameworks/raw/master/IntroductionDeepReinforcementLearning/keras_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To plot figures and animations\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "#Tensorflow/Keras utils\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Library\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions enable to build a video from a list of images. <br>\n",
    "They will be used to build video of the game you will played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# AI Gym Librairie\n",
    "<a href=\"https://gym.openai.com/\" ><img src=\"https://gym.openai.com/assets/dist/home/header/home-icon-54c30e2345.svg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple environment: the Cart-Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### Observation\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the righ&t\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between ±0.05\n",
    "\n",
    "### Episode Termination\n",
    "1. Pole Angle is more than ±12°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "The description above if part of the official description of this environemtn. Read full description [here](https://github.com/openai/gym/wiki/CartPole-v0).\n",
    "\n",
    "The following command will load the `CartPole` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "print(\"Environemnt is a %dx%dx%d images\" %img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have forgotten how the `CartPole` environment works, open the `Deep_Q_Learning_CartPole.ipynb` notebook to run explanation's cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard coded policy\n",
    "\n",
    "How can we make the poll remain upright? We will need to define a _policy_ for that. \n",
    "\n",
    "This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do.\n",
    "\n",
    "Let's first implement **Hard Coded policies**, *i.e.* simple rules that defines which action to takes according to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode(policy, return_frames=False):\n",
    "    frames = []\n",
    "    observation = env.reset()    \n",
    "    reward_episod = 0    \n",
    "    done = False\n",
    "    while not(done):\n",
    "        action = policy(observation)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward_episod += reward\n",
    "        if return_frames:\n",
    "            img = env.render(mode = \"rgb_array\")\n",
    "            env.close()\n",
    "            frames.append(img)\n",
    "    return reward_episod, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(policy, n_games=100):\n",
    "    all_reward_sum = []   \n",
    "    n_game = 0          \n",
    "    while n_game < n_games:\n",
    "        reward_episod, _ = run_one_episode(policy)\n",
    "        if n_game %10 == 0:\n",
    "            print(\"Game played : %d. Mean and Standart deviation's reward for the last 10 episode: %.1f - %.1f\" %(n_game, np.mean(all_reward_sum[-10:]), np.std(all_reward_sum[-10:])) )\n",
    "        all_reward_sum.append(reward_episod)\n",
    "        n_game += 1\n",
    "    print(\"Over %d episodes, mean reward: %.1f, std : %.1f\" %(n_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a completly random policy and see how much time the poll will remain upright over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_random(state):\n",
    "    return env.action_space.sample()\n",
    "play_games(policy = policy_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a complete game\n",
    "\n",
    "Let's run one pisode with his random policy and save all images representing the environment at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episod, frames = run_one_episode(policy = policy_random, return_frames=True)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and _vice versa_. Let's see if that works.\n",
    "\n",
    "**Exercise** implement this policy and play 100 games with this policy. What are the means and std deviation of the reward sum over the 100 games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pg_simple_policy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_games(policy = simple_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What can you say about this strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Vizualize a complete game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episod, frames = run_one_episode(policy = simple_policy, return_frames=True)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network to build a better policy.\n",
    "\n",
    "This network will take the observations as inputs, and output the probability of the action to take for each observation. <br>\n",
    "\n",
    "In the case of the Cart-Pole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability `p` of the action 1 (right), and of course the probability of action 0 (right) will be `1 - p`.\n",
    "\n",
    "Let's first see how this neural network policy work without training it and then let's try to learn the simple policy define above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this problem is simple, we can define a very simple architecture for our neural network.<bR> \n",
    "    \n",
    "Here it's simple MLP with 1 hidden layer and four neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the network architecture\n",
    "n_inputs = 4  # == env.observation_space.shape[0]\n",
    "n_hidden = 9  # it's a simple task, we don't need more than this\n",
    "n_outputs = 1 # only outputs the probability of accelerating right\n",
    "\n",
    "# Build the neural network\n",
    "policy_network=km.Sequential()\n",
    "policy_network.add(kl.Dense(n_hidden, input_shape = (n_inputs,), activation = \"relu\"))\n",
    "policy_network.add(kl.Dense(n_outputs, activation = \"sigmoid\"))\n",
    "policy_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model is not compile so far, no loss function is defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy from a neural network\n",
    "We can now easily predict the probability of both actions given the observation.\n",
    "\n",
    "**Exercise** Define a function to choose the action to take from an observation and the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pg_neural_network_policy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** :  In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment's full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. \n",
    "\n",
    "For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. <br> Another example:  if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment's full state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random neural network policy.\n",
    "Let's see how this neural network policy perform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_games(policy = lambda obs : neural_network_policy(obs, model=policy_network), n_games=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly initialize this policy neural network and use it to play one game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episod, frames = run_one_episode(policy = lambda obs : neural_network_policy(obs, model=policy_network), return_frames=True)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is working. But it's still acting randomly because we do not train the neural network. Let's try to make it learn better policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a given policy\n",
    "\n",
    "In this part we will train the neural network in order that it learns the simple strategy we hard coded before : if the pole is tilting to the left, then push the cart to the left, and _vice versa_. <br>\n",
    "\n",
    "\n",
    "The class defined below enables to train the neural network in order to learn this simple policy.\n",
    "\n",
    "The **pseudo code** is quite simple here:\n",
    "\n",
    "while *n_episode_max* is not reached:\n",
    "    * Play and episode and return the observation and target\n",
    "    * Train the network from these observation and target. \n",
    "\n",
    "\n",
    "**Exercicse**: Complete the code below: <br>\n",
    "* Choose a loss to compile the model with in the `init_model`method.\n",
    "* Write the `play_one_episode`method that will: \n",
    "    * Play an episode until it's end with the hard coded policy\n",
    "    * Return observation an target, i.e, the action to take according to each observation, of this episode. \n",
    "    \n",
    "You can do this exercise on this notebook or with the `PG_learn_a_policy.py`and the `PG_learn_a_policy_solution.py` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Environment\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.dim_input = self.env.observation_space.shape[0]\n",
    "\n",
    "        # Model\n",
    "        self.model = self.init_model()\n",
    "        self.n_episode_max = 1000\n",
    "\n",
    "    def init_model(self):\n",
    "\n",
    "        # Build the neural network\n",
    "        policy_network = km.Sequential()\n",
    "        policy_network.add(kl.Dense(9, input_shape=(self.dim_input,), activation=\"relu\"))\n",
    "        policy_network.add(kl.Dense(1, activation=\"sigmoid\"))\n",
    "        policy_network.compile(loss=, optimizer=ko.Adam(), metrics=['accuracy'])\n",
    "        return policy_network\n",
    "\n",
    "    def play_one_episode(self):\n",
    "        # Todo\n",
    "        return train_data\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for iteration in tqdm(range(self.n_episode_max)):\n",
    "            train_data = self.play_one_episode()\n",
    "            n_step = len(train_data)\n",
    "            target = np.array([x[1] for x in train_data]).reshape((n_step, 1))\n",
    "            observations = np.array([x[0] for x in train_data])\n",
    "            self.model.train_on_batch(observations, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PG()\n",
    "pg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pg_learn_given_policy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_games(policy = lambda obs : neural_network_policy(obs, model=pg.model), n_games=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episod, frames = run_one_episode(policy = lambda obs : neural_network_policy(obs, model=pg.model), return_frames=True)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it learned the policy correctly! <br>\n",
    "\n",
    "Let's now reach our final target : The neural network has to find a better policy by its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind *Policy Gradients* its quite simple : The _Policy Gradients_ algorithm tackles this problem by first playing multiple games, then making the actions in good games slightly more likely, while actions in bad games are made slightly less likely. First we play, then we go back and think about what we did.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "* Run an episode untill it's done and save at each iteration the observation, action and reward.\n",
    "* When an episode it's done. Compute the discounted rewards for all the episode, and save it.\n",
    "* If you have done *batch_size=50* episodes train your model on this batch.\n",
    "* Stop if you have reach *num episodes* or *goal* target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameters\n",
    "\n",
    "| Variable  | Value  | Description  | \n",
    "|---|---|---|\n",
    "|Gamma   | 0.99  | The discounted rate apply for the discounted reward  |\n",
    "|batch_size  | 50   | Number of episode to run before training model on a batch of episode  |\n",
    "| Num episodes | 10.000   | Maximum number of episode to run before stopping the training  | \n",
    "| goal | 190  | Number of step to achieve on one episode to stop the training.  |\n",
    "\n",
    "Those parameters are fixed for this TP, they are common value for this kind of problem based on experiences. They are not definitive nor results or any research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this neural network we will then used the observation of the experiences as an inputs and the actions taken as an output.\n",
    "\n",
    "But how do we provide to the neural network the information the choosen actions  was good or bad?\n",
    "The problem is that most actions have delayed effects, so when you win or lose points in a game, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? <br>\n",
    "This is called the _credit assignment problem_.\n",
    "\n",
    "\n",
    "To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that come after it, usually applying a discount rate r at each step. It's call the **discounted rewards**\n",
    "\n",
    "$$ R_t = \\sum_{i=0}^{\\infty}\\gamma^i r_{t+i}$$\n",
    "\n",
    "\n",
    "\n",
    "This rate will the be applied to the loss function of the neural network :\n",
    "* A high discounted reward will lead to higher gradient which will increase the importance of this action\n",
    "* A low  discounted reward will lead to lower gradient which will decrease the importance of this action\n",
    " \n",
    "\n",
    "**Exercise** : Implement the discount_rewards function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load discounted_rewards.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(discount_rewards([1,1,1], gamma=0.99) == [2.9701, 1.99, 1])\n",
    "assert np.all(discount_rewards([3,2,1],gamma=0.99) == [5.960100000000001, 2.99, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture & Loss Function\n",
    "\n",
    "As before we will define a very simple architecture to our neural network : A MLP with only one hidden layer and 8 neurons.\n",
    "\n",
    "We have to be aware here that the neural network will have two different behaviour :\n",
    "\n",
    "* For training: the model will take two information as an input : The observations (to predict the action), and the discounted rate (also call advantages) that will be applied on the loss function.\n",
    "* For prediction : the model will take only the observations as an input to predict the action.\n",
    "\n",
    "So we have to define a neural network that can either handle one or two inputs! \n",
    "\n",
    "In keras we define it that way : we define the layers, and we create two models (for training and prediction) that will share the same layers and weight.\n",
    "\n",
    "We will also implement the loss function, which is weighted binary cross entropy, where the weight are the discounted rated computed from the rewards\n",
    "\n",
    "Here is how we implement it : (Make sure you understand it!) \n",
    "\n",
    "**TODO** Define this keras Model class with tensorflow 2\n",
    "\n",
    "**Exercise**: Write the loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as klo\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.metrics as kme\n",
    "\n",
    "class discountedLoss(klo.Loss):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      pos_weight: Scalar to affect the positive labels of the loss function.\n",
    "      weight: Scalar to affect the entirety of the loss function.\n",
    "      from_logits: Whether to compute loss from logits or the probability.\n",
    "      reduction: Type of tf.keras.losses.Reduction to apply to loss.\n",
    "      name: Name of the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 reduction=klo.Reduction.AUTO,\n",
    "                 name='discountedLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred, adv):\n",
    "        log_lik = - (y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "        loss = K.mean(log_lik * adv, keepdims=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class kerasModel(km.Model):\n",
    "    def __init__(self):\n",
    "        super(kerasModel, self).__init__()\n",
    "        self.layersList = []\n",
    "        self.layersList.append(kl.Dense(9, activation=\"relu\",\n",
    "                     input_shape=(4,),\n",
    "                     use_bias=False,\n",
    "                     kernel_initializer=ki.VarianceScaling(),\n",
    "                     name=\"dense_1\"))\n",
    "        self.layersList.append(kl.Dense(1,\n",
    "                       activation=\"sigmoid\",\n",
    "                       kernel_initializer=ki.VarianceScaling(),\n",
    "                       use_bias=False,\n",
    "                       name=\"out\"))\n",
    "\n",
    "        self.loss = discountedLoss()\n",
    "        self.optimizer = ko.Adam(lr=1e-2)\n",
    "        self.train_loss = kme.Mean(name='train_loss')\n",
    "        self.validation_loss = kme.Mean(name='val_loss')\n",
    "        self.metric = kme.Accuracy(name=\"accuracy\")\n",
    "\n",
    "        @tf.function()\n",
    "        def predict(x):\n",
    "            \"\"\"\n",
    "            This is where we run\n",
    "            through our whole dataset and return it, when training and testing.\n",
    "            \"\"\"\n",
    "            for l in self.layersList:\n",
    "                x = l(x)\n",
    "            return x\n",
    "        self.predict = predict\n",
    "\n",
    "        @tf.function()\n",
    "        def train_step(x, labels, adv):\n",
    "            \"\"\"\n",
    "                This is a TensorFlow function, run once for each epoch for the\n",
    "                whole input. We move forward first, then calculate gradients with\n",
    "                Gradient Tape to move backwards.\n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.predict(x)\n",
    "                loss = self.loss.call(\n",
    "                    y_true=labels,\n",
    "                    y_pred = predictions,\n",
    "                    adv = adv)\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            self.train_loss(loss)\n",
    "            return loss\n",
    "\n",
    "        self.train_step = train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG class\n",
    "\n",
    "\n",
    "The `PG` class contains the implementation of the **Policy Gradient** algorithm. The code is incomplete and you will have to fill it!\n",
    "\n",
    "**GENERAL INSTRUCTION**:\n",
    "\n",
    "* Read the init of the `PG` class. \n",
    "    * Various variable are set with their definition, make sure you understand all of its.\n",
    "    * The *game environment*, the *experiences list* and the *keras model* are initialised.\n",
    "* Read the `train` method. It contains the main code corresponding to the **pseudo code** below. YOU DO NOT HAVE TO MODIFY IT! But make sure you understand it.\n",
    "* The `train` method use methods that are not implemented. \n",
    "    * You will have to complete the code of 3 functions. (read instruction of each exercise below)\n",
    "    * After the cell of the `PG` class code below there is a **test cells**. <br>\n",
    "    This cell should be executed after all the methods have been completed. This cell will check that the function you implemented take input and output in the desired format. <br> DO NOT MODIFY this cell. It will work if you're code is good <br> **Warning** The test cell does not guarantee that your code is correct. It just test than input and output are in the good format.\n",
    "\n",
    "\n",
    "#### Pseudo code \n",
    "We will consider that the game is **completed** if the mean score over 10 games is above 190.\n",
    "\n",
    "While you didn't reach the expected *goal* reward or the max *num_episodes* allow to be played:\n",
    "* Run `one_episode` and save all experiences in the `experiences` list (**Exercise 1 & 2**):\n",
    "* Every `batch_size` episodes played:\n",
    "    * train model over a batch of experiences (**Exercise 3**)\n",
    "\n",
    "\n",
    "    \n",
    "**Exercise 1**:  Implement `choose_action`<br>\n",
    "This method chooses an action according to this rules:<br>\n",
    "\n",
    "* let $p$ be the probability of the output of the model that we play the action $right(=1)$,\n",
    "* Then choose action to play right with probability $p$ else play left.\n",
    "* Hence, the more the model will be good about it's prediction, the less exploration we will perform.\n",
    "\n",
    "\n",
    "**Exercise 2**:  Implement `run_one_episode` <br>\n",
    "This method:<br>\n",
    "* play an complete episode until it's done. At each step of the episode, it :\n",
    "    * chooses an action\n",
    "    * save all the actions, state and reward\n",
    "* once the episode is done and all rewards are know, it compute all the discounted rewards.\n",
    "* fill the `experiences's` list with all experience of the episode = '[state, action, discounted_reward]'\n",
    "\n",
    "**Exercise 3**:  Implement `run_one_batch_train`<br>\n",
    "This method:<br>\n",
    "* call the on `train_step` method of the `model`with the argument in the `experiences` list.\n",
    "* Empty the `experiences` list\n",
    "* return the loss of this batch step.\n",
    "\n",
    "\n",
    "You can do these exercises on this notebook or with the `PG.py`and the `PG_solution.py` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "class PG:\n",
    "\n",
    "    def __init__(self, gamma=.99, batch_size=50, num_episodes=10000, goal=190, n_test=10, print_every=100):\n",
    "        # Environment\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.dim_input = self.env.observation_space.shape[0]\n",
    "\n",
    "        # Parameters\n",
    "        self.gamma = gamma  # -> Discounted reward\n",
    "        self.batch_size = batch_size  # -> Size of episode before training on a batch\n",
    "\n",
    "        # Stop factor\n",
    "        self.num_episodes = num_episodes  # Max number of iterations\n",
    "        self.goal = goal  # Stop if our network achieve this goal over *n_test*\n",
    "        self.n_test = n_test\n",
    "        self.print_every = print_every  # ?Numbe rof episode before trying if our model perform well.\n",
    "\n",
    "        # Init Model to be trained\n",
    "        self.model = kerasModel()\n",
    "\n",
    "        # Placeholders for our observations, outputs and rewards\n",
    "        self.experiences = []\n",
    "        self.losses = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # TODO\n",
    "        return action\n",
    "\n",
    "    def run_one_episode(self):\n",
    "        # TODO\n",
    "        return score\n",
    "\n",
    "    def run_one_batch_train(self):\n",
    "        # TODO\n",
    "        return loss\n",
    "\n",
    "    def score_model(self, model, num_tests, dimen, ):\n",
    "        scores = []\n",
    "        for num_test in range(num_tests):\n",
    "            observation = self.env.reset()\n",
    "            reward_sum = 0\n",
    "            while True:\n",
    "                state = np.reshape(observation, [1, dimen])\n",
    "                predict = model.predict(state)[0]\n",
    "                action = 1 if predict > 0.5 else 0\n",
    "                observation, reward, done, _ = self.env.step(action)\n",
    "                reward_sum += reward\n",
    "                if done:\n",
    "                    break\n",
    "            scores.append(reward_sum)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def train(self):\n",
    "        metadata = []\n",
    "        i_batch = 0\n",
    "        # Number of episode and total score\n",
    "        num_episode = 0\n",
    "        train_score_sum = 0\n",
    "\n",
    "        while num_episode < self.num_episodes:\n",
    "            train_score = self.run_one_episode()\n",
    "            train_score_sum += train_score\n",
    "            num_episode += 1\n",
    "\n",
    "            if num_episode % self.batch_size == 0:\n",
    "                i_batch += 1\n",
    "                loss = self.run_one_batch_train()\n",
    "                self.losses.append(loss)\n",
    "                metadata.append([i_batch, self.score_model(self.model, self.n_test, self.dim_input)])\n",
    "\n",
    "            # Print results periodically\n",
    "            if num_episode % self.print_every == 0:\n",
    "                test_score = self.score_model(self.model, self.n_test, self.dim_input)\n",
    "                print(\n",
    "                    \"Average reward for training episode {}: {:0.2f} Mean test score over {:d} episode: {:0.2f} Loss: {:0.6f} \".format(\n",
    "                        num_episode, train_score_sum / self.print_every, self.n_test,\n",
    "                        test_score,\n",
    "                        self.losses[-1]))\n",
    "                reward_sum = 0\n",
    "                if test_score >= self.goal:\n",
    "                    print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                    break\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PG()\n",
    "score = pg.run_one_episode()\n",
    "assert type(score) is float\n",
    "for state, action, dreward in pg.experiences:\n",
    "    assert np.all(state.shape==(1,4))\n",
    "    assert type(action)==int\n",
    "    assert type(dreward)==np.float64\n",
    "\n",
    "loss = pg.run_one_batch_train()\n",
    "assert type(loss) == float\n",
    "assert len(pg.experiences)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/PG_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PG(goal=200)\n",
    "metadata = pg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(list(range(len(metadata))),[x[1] for x in metadata])\n",
    "ax.set_yticks(np.arange(0,210,10))    \n",
    "ax.set_xticks(np.arange(0,100,25))    \n",
    "ax.set_title(\"Score/Lenght of episode over Iteration\", fontsize=20)\n",
    "ax.set_xlabel(\"Number of iteration\", fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "ax.set_ylabel(\"Score/Length of episode\", fontsize=16)\n",
    "plt.savefig(\"pg_normalized.png\", bbox_to_anchor=\"tigh\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "* Use the model to play 100 games and check how it performs compare to previous policy tested\n",
    "* Register a video of a game and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_games(policy = lambda obs : neural_network_policy(obs, model=pg.model), n_games=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episod, frames = run_one_episode(policy = lambda obs : neural_network_policy(obs, model=pg.model), return_frames=True)\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
