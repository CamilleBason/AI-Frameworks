{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data : Cdiscount's product description.\n",
    "\n",
    "This dataset has been released from Cdiscount for a data competition (type kaggle) on the french website [datascience.net](https://www.datascience.net/fr/challenge). <br>\n",
    "The test dataset of this competition has not been released, so we used a subset of 1M producted of the original train dataset(+15M rows) all along the **Natural Language Processing** lab.<br>\n",
    "The objective of this competition was to classify the text description of various product into various categories that compose the navigation tree of Cdiscount website. It is composed of 4,733 categories organized within 44 meta categories. <br>\n",
    "\n",
    "The objective of this lab is not win the competition so we will only used the meta-categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Recurrent Neural Network. Application to text classification and text generation\n",
    "\n",
    "In this second notebook we study how to use a recurrent neural network (RNN) for two use case:\n",
    "\n",
    "* Text classification : As the two precedent notebook, we will use Recurrent Neural Network algorithm to predict product's category.\n",
    "* Text Generation: We will see how to generate product description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo \n",
    "* Text generation is one to many\n",
    "* add classification (many to one)\n",
    "* marque prediction? many to many?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "\n",
    "This part aims to understand how to build the different types of RNN models (**one/many-to-one/many**) with `keras`.\n",
    "\n",
    "The example are stricly pedagogical and wouldn't need such a models to be built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to one\n",
    "\n",
    "**Many-to-one** recurrent neural network take a sequence as an input and return a scalar as an output :\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_one.png?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example\n",
    "\n",
    "Let's take an example where the *input* are sequences of 3 numbers and the output the sum of these 3 numbers.\n",
    "\n",
    "Hence the dimensions of the input matrix *X* will be of size:\n",
    "\n",
    "* N: Number of sequences = 100 (arbitrary values), \n",
    "* Timestep: (Size of sequences) = 3, \n",
    "* Number of features (How many features for each element of the sequences) = 1 \n",
    "\n",
    "*NB*: by default, keras handle the dimensions of the input in that order : N_batch, Timestep and Features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(300).reshape(100,3,1)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %X.shape)\n",
    "Y = X.sum(1).reshape(100,1,1)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %Y.shape)\n",
    "print(\"Input Example\")\n",
    "print(X[0])\n",
    "print(\"Output Example\")\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "The following lines enable to define a very simple model with one **many-to-one** *RNN* layer with:\n",
    "\n",
    "* 10 neurons (*units=10)\n",
    "* a *relu* activation layer\n",
    "\n",
    "This model take as an input sequences of size (3,1). Note that the *input_shape* argument does not take the batch size as an input. Only the *timestep* and the *feature sie*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Do the shape of the output seems normal to you? What do the two dimensions represent? <br>\n",
    "**Exercise** : Send a sequence trough the model and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/simple_rnn_output.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the complete model. As the output of the RNN layer is a vector with 10 features, let's add a Dense layer so that the output is of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1)))\n",
    "model.add(kl.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model with and *adam* optimizer and a *mse* as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model can now correctly perform the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([10,11,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([10,25,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if  input_shape = (1,1) we set a **one-to-one** recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With no fix timestep\n",
    "\n",
    "Note that in the previous example, the timestep was fix to three. But it's possible to set the parameters to *None* so that the model can handle sequences of variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(None, 1)))\n",
    "model.add(kl.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the model can handle sequence of variable lengths, during training, all sequences should have same lengths.\n",
    "\n",
    "To handle this, we apply zero padding to the sequences of variables length so that it does not affect the results thanks to the `pad_sequences` function from `keras`.\n",
    "\n",
    "Let's first create a X list of sequences of different size (3 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in np.arange(0,300,3):\n",
    "    X.append([x,x+1,x+2])\n",
    "for x in np.arange(300,500,2):\n",
    "    X.append([x,x+1])\n",
    "collections.Counter([len(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pad this sequence with zero values. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, value=0.0, padding = 'pre')\n",
    "print(\"X shape : (%d,%d)\" %X.shape)\n",
    "print(\"3 first sequences\")\n",
    "print(X[:2])\n",
    "print(\"2 last sequences\")\n",
    "X[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a good practices aims to pad value to the **left** of the sequences. <br>\n",
    "This can be not intuitive but The reason is that nothing is learn as the beginning of the sequence because all the values would be zeros, the real learning would start when first non zeros values appears. <br>\n",
    "If the sequences are padded to the right, the information learn on the beginning of the sequences could be lost passing through all zeros values at the end of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(200,3,1)\n",
    "Y = X.sum(1)\n",
    "\n",
    "epochs = 500\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict sum of sequences of different lenght (even bigger sequences, but results is not guaranted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([3,4]).reshape(1,2,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([3,4,5]).reshape(1,3,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([3,4,5,6]).reshape(1,4,1)\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to Many\n",
    "\n",
    "**Many-to-many** recurrent neural network take a sequence as an input and return a sequence as an output :\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example\n",
    "\n",
    "Let's take an example where the *input* are sequences of 3 number  and the output will be a sequences of cumulative sum, i.e.\n",
    "\n",
    "* input = [x1, x2, x3]\n",
    "* output = [x1, x1+x2, x1+x2+x3]\n",
    "\n",
    "\n",
    "Hence BOTH the dimensions of the input *X* AND the output *Y*  matrices will be of size:\n",
    "\n",
    "* N: Number of sequences = 100 (arbitrary values), \n",
    "* Timestep: (Size of sequences) = 3, \n",
    "* Number of features (How many features for each element of the sequences) = 1,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(300).reshape(100,3,1)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %X.shape)\n",
    "Y = X.cumsum(1).reshape(100,3,1)\n",
    "print(\"Dimensions of input sequences: %d, Timestep: %d, Number of features: %d\" %Y.shape)\n",
    "print(\"Input Example\")\n",
    "print(X[0])\n",
    "print(\"Output Example\")\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "The following lines enable to define a very simple model with one **many-tom-many** *RNN* layer with:\n",
    "\n",
    "* 10 neurons (*units=10)\n",
    "* a *relu* activation layer\n",
    "\n",
    "This model take as an input sequences of size (3,1) and return a sequence of the same size.<br>\n",
    "This is specified but the *return_sequences* argument wich is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Do the shape of the output seems normal to you? What do the three dimensions represent? <br>\n",
    "**Exercise** : Send a sequence trough the model and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/simple_rnn_output_bis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the complete model. <br>\n",
    "For each input sequences, the output of the RNN layer is a matrix of size 3 (number of timestep) per 10  (features). <br>\n",
    "The desired output would be a sequence of size 3  per 1.\n",
    "\n",
    "In order to obtain the correct dimension let's add a Dense layer at each timestep of the output wit the help of the `TimeDistributed` layer of `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=10 ,activation=\"relu\", input_shape=(3, 1), return_sequences=True))\n",
    "model.add(kl.TimeDistributed(kl.Dense(1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model with and *adam* optimizer and a *mse* as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size=32\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model can now correctly perform the cumulative sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array([10,11,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))\n",
    "x_test=np.array([10,25,12]).reshape(1,3,1)\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as previously seen, it would been possible to set the *timestep* parameters to *None* so that the model can compute cumulative sum of model whatever the size of their length.  \n",
    "This could be a good **exercise** if you want to practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One to Many\n",
    "\n",
    "**One-to-many** recurrent neural network take a scalar as an input and return a sequence as an output. \n",
    "\n",
    "There are different ways to define **one-to-many**\n",
    "neural network. \n",
    "\n",
    "* In the example below, the **one-to-many** network can be seen as as a **many-to-many** neural network where the input sequence is build iteratively.\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/one_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "* It would also be possible to only pass an input at the first timestep. Then **one-to-many** network can be seen as as a **many-to-many** neural network where the input sequence is composed of one scalar and `None` entry to fill the sequences. \n",
    "\n",
    "Hence, **one-to-many** networks, can be seen as particular case of **many-to-many** neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example\n",
    "\n",
    "Let's take an example where the *input* are scalar and the output is sequence of 3 number composed such that\n",
    "\n",
    "* input = x\n",
    "* output = [x+2, x+4, x+6]\n",
    "\n",
    "Hence the dimensions of the output matrix *Y* will be of size:\n",
    "\n",
    "* N: Number of sequences = 100 (arbitrary values), \n",
    "* Timestep: (Size of sequences) = 3, \n",
    "* Number of features (How many features for each element of the sequences) = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "At **training**  the keras model will be built as a **many-to-many** models. <br>\n",
    "Indeed as you now the sequence output you're expect to get, you now the sequence that will be send as an input you want to learn. IN the example above:\n",
    "\n",
    "* input = [x,x+2,x+4].\n",
    "* output = [x+2,x+4,x+6]\n",
    "\n",
    "**Exercise**: Build the toy dataset and the models that will learn how to predict the output sequences from and input sequences.<br>\n",
    "**nb** Remember that at prediction, the model should be able to take a scalar as an input (i.e. a sequence of one timestep).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/one_to_many_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/one_to_many_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Once your model is build, write a function that build a the 3 numbers sequences output from a scalar input using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/one_to_many_prediction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the examples details so far have treated *one-size* features sequences in order to make this tutorial easier. \n",
    "\n",
    "All of these examples can be easily traduced to *several-size* features length. Let's check that with example on the **Cdisocunt** Dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN layers\n",
    "\n",
    "Once you know how to manipulate the structure defined above, it is really easy to build more complex or deepest RNN model with keras.\n",
    "\n",
    "* `GRU` and `LSTM` can be used the exact same way than `SimpleRNN`in the example above.\n",
    "* Bi-directional layers can be build using the `Bidirectional` layer on `RNN`layer.\n",
    "* Deep RNN can be build adding `RNN` layer like any other sequential model.\n",
    "\n",
    "***Example***:\n",
    "Here is how to build a model with one *LSTM* layer follow by a bidirectional *GRU* layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.LSTM(units=10 ,activation=\"relu\", input_shape=(3, 1), return_sequences=True))\n",
    "model.add(kl.Bidirectional(kl.GRU(units=10 ,activation=\"relu\", return_sequences=True)))\n",
    "model.add(kl.TimeDistributed(kl.Dense(1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The Level 3 category `COQUE - BUMPER - FACADE TELEPHONE` is the most represented category within the original **Cdiscount**'s dataset with 2.184.671 descriptions. Among them 1.761.637 are composed with 197 characters. \n",
    "\n",
    "We will now use these lines (or sub-samble of these lines according to the computation power of your machine) in order to learn a text generation model that will allow to automatically generate a new text description of this type of product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "X = np.load(\"data/description_coque.npy\")[:N]\n",
    "print(X.shape)\n",
    "print(X[:3])\n",
    "print(\"Size of all the sequences : %s\" %(str(set([len(x) for x in X]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns=197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The text generation implies to build a **one-To_many** model:\n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/one_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Where the prediction $y_t$ will be used as an input at time $t+1$, i.e : $y_t=x_{t+1}$. \n",
    "\n",
    "This model with be trained as a **many-to-many**  model. \n",
    "\n",
    "<img src=\"https://github.com/wikistat/AI-Frameworks/blob/master/Text/images/many_to_many.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Where the output y will be the same sequence than input x with 1 offset.\n",
    "\n",
    "When using text, the input can be either a word or a characters. As sequences have fixed length, we will use the characters as inputs of the sequences. <br>\n",
    "These characters will be *one-hot encoded* <br>\n",
    "\n",
    "Hence each description $x$ will be represented as a Matrix of size $N_s \\times N_v$ where\n",
    "\n",
    "* $N_s=197$ is the length of the sequences (timestep)\n",
    "* $N_v$ is the size of the vocabulary (the list of caracters) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters' list\n",
    "\n",
    "Let's first create a list of unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_set = list(functools.reduce(lambda x,y : x.union(y), [set(x) for x in X], set()))\n",
    "print(\"Characters list of size %d : %s\"  %(len(chars_set), str(chars_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add two elements to these listes allowing to detect the *start* and the *end* of a sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_set.extend([\"START\",\"END\"])\n",
    "Nv = len(chars_set)\n",
    "print(\"Total size of the vocabulary : %d\" %Nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence encoding\n",
    "\n",
    "There are no library (or I do not find it), that enable to *one-hot encode* a string at a character level.\n",
    "\n",
    "The following lines enables to apply it.\n",
    "\n",
    "* First `char_to_int` and `int_to_char` dictionary are created, enabling to retrieve the position of a character in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {i:c for i,c in enumerate(chars_set)}\n",
    "char_to_int = {c:i for i,c in int_to_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function encode\n",
    "\n",
    "* a  $X\\in \\mathbb{R}^{N \\times N_d}$ matrix composed of *N* text description of size *N_s*   size  \n",
    "into \n",
    "* a $X_{vec} \\in \\mathbb{R}^{N \\times N_d \\times N_v}$ matrix composed of *N* sequences of size $N_s\\times N_v$ (the encoded text description) .\n",
    "\n",
    "and the $Y\\in \\mathbb{R}^{N \\times N_d}$ matrix (which is the same that the $X$ matrix with offset one) to the $Y_{vec} \\in \\mathbb{R}^{N \\times N_d \\times N_v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_output_sequence(x_descriptions, length_sequence, size_vocab, char_to_int_dic):\n",
    "    # Get the number of description in x.\n",
    "    n = x_descriptions.shape[0]\n",
    "    \n",
    "    # Set the dimensions of the output encoded matrices fill with zero.\n",
    "    # the length_sequence is actually length_sequences\n",
    "    x_vec = np.zeros((n,length_sequence+1, size_vocab))\n",
    "    y_vec = np.zeros((n,length_sequence+1, size_vocab))\n",
    "    \n",
    "    \n",
    "    # Let's now fill the matrices with one at the location of each characters position\n",
    "    \n",
    "    # First let's fill each input sequences with the START position at the begining of the encoded sequences\n",
    "    x_vec[:,0,char_to_int[\"START\"]] = 1\n",
    "    # and the output sequences with the END position at the end of the encoded sequences\n",
    "    y_vec[:,-1,char_to_int[\"END\"]] = 1\n",
    "    # Now let's iterate over all x_descriptions\n",
    "    for ix,x in tqdm(enumerate(x_descriptions)):\n",
    "        # And over each character of the description\n",
    "        for ic,c in enumerate(x):\n",
    "            # For each character `c` we set one at his position in the vocabulary.\n",
    "            c_int = char_to_int_dic[c]\n",
    "            x_vec[ix,ic+1,c_int]=1\n",
    "    # The y-vec matrices is the same than the x matrix with one offset\n",
    "    y_vec[:,:-1,:] = x_vec[:,1:,:] \n",
    "    return x_vec, y_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Be sure to understand each step of these function.\n",
    "\n",
    "Let's apply it on the Descriptions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec, Y_vec = encode_input_output_sequence(X[:N], Ns, Nv, char_to_int)\n",
    "X_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** \n",
    "* Write a function that enables to retrieve the original phrase from an encoded sequences.\n",
    "* Check that *x* and *y* have actually the same description with one offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/decoded_vector.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "**Exercise**: Define a simple model (only one LSTM layer with 32 hidden units) that will allow to train the text generation model.\n",
    "*Tips*:\n",
    "* Remember that this model will be used for generation.\n",
    "* What are the dimension of the output? What will be the activation layer? The loss function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/train_model_text_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have correctly write the model you can observe that it can take a while to obtain convergence when training thise kind of model.\n",
    "\n",
    "Let's download this model, generated with the solution above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"data/generate_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below enable to predict a sentence by iterativaly predict a character sending a previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor\n",
    "x_pred = np.zeros((1, Ns+1, Nv))\n",
    "print(\"step 0\")\n",
    "x_pred[0,0,char_to_int[\"START\"]] =1\n",
    "x_pred_str = decode_sequence(x_pred[0], int_to_char)\n",
    "print(x_pred_str)\n",
    "\n",
    "for i in range(Ns):\n",
    "    x_tensor = convert_to_tensor(x_pred[:,:i+1,:])\n",
    "    ix = np.argmax(model.predict(x_tensor)[0][-1,:])\n",
    "    x_pred[0,i+1,ix] = 1\n",
    "x_pred_str=decode_sequence(x_pred[0], int_to_char)\n",
    "print(x_pred_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** How this prediction is done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Generate a text generation with random first letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/text_generation_random_first_letter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Generate a text generation with some randomness. For example, use a multinomial from the model output to generate a characters at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/text_generation_multinomial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sms\n",
    "from solution.clean import CleanText\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vectorizer import Vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "data = pd.read_csv(\"data/cdiscount_train.csv.zip\",sep=\",\", nrows=100000)\n",
    "ct.clean_df_column(data, \"Description\", \"Description_cleaned\")\n",
    "print(\"The train dataset is composed of %d lines\" %data.shape[0])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"data/cdiscount_test.csv.zip\",sep=\",\")\n",
    "ct.clean_df_column(data_test, \"Description\", \"Description_cleaned\")\n",
    "print(\"The train dataset is composed of %d lines\" %data_test.shape[0])\n",
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = sms.train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def vect_sequence(data_array):\n",
    "    data_array = [line.split(\"\") for line in data_array[\"Description_cleaned\"].values]\n",
    "    vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "    data_vec = vec.fit_transform(data_array)\n",
    "    \n",
    "    return data_vec\n",
    "\n",
    "data_vec_train = vect_sequence(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_method = Vectorizer(vectorizer_type = \"tfidf\", nb_hash = None )\n",
    "vec, feathash, data_train_vec = vect_method.vectorizer_train(data_train, columns = \"Description_cleaned\")\n",
    "data_valid_vec = vect_method.apply_vectorizer(data_valid, columns = \"Description_cleaned\", vec = vec, feathash = feathash)\n",
    "data_test_vec = vect_method.apply_vectorizer(data_test, columns = \"Description_cleaned\", vec = vec, feathash = feathash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_label = {k:v for k,v in enumerate(set(Y_train))}\n",
    "label_to_int = {v:k for k,v in int_to_label.items()}\n",
    "Y_train = data_train.Categorie1.values\n",
    "Y_train_int = np.array([label_to_int[y] for y in Y_train]).reshape(len(Y_train),1)\n",
    "Y_valid = data_valid.Categorie1.values\n",
    "Y_valid_int = np.array([label_to_int[y] for y in Y_valid]).reshape(len(Y_valid),1)\n",
    "Y_test = data_test.Categorie1.values\n",
    "Y_test_int = np.array([label_to_int[y] for y in Y_test]).reshape(len(Y_test),1)\n",
    "N_label = len(int_to_label)\n",
    "print(N_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.SimpleRNN(units=100 ,activation=\"relu\", input_shape=(28, 300), return_sequences=True))\n",
    "model.add(kl.BatchNormalization())\n",
    "model.add(kl.SimpleRNN(units=50 ,activation=\"relu\"))\n",
    "model.add(kl.Dense(N_label))\n",
    "model.add(kl.Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "epochs = 500\n",
    "batch_size=256\n",
    "history = model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history=model.fit(X_train, Y_train_int, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=[X_valid, Y_valid_int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_token = [line.split(\" \") for line in data_train[\"Description_cleaned\"].values]\n",
    "valid_array_token = [line.split(\" \") for line in data_valid[\"Description_cleaned\"].values]\n",
    "test_array_token = [line.split(\" \") for line in data_test[\"Description_cleaned\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg_full = gensim.models.Word2Vec.load(\"data/w2v_model/full_model_sg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile([len(x) for x in train_array_token], q=[99]),max([len(x) for x in train_array_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def tokens_to_embedding_sequences(array_token, model):\n",
    "    array_embedding_sequences = []\n",
    "    for tokens in tqdm(array_token):\n",
    "        embedding_sequence = []\n",
    "        for token in tokens[:28]:\n",
    "             embedding_sequence.append(model[token])\n",
    "        array_embedding_sequences.append(embedding_sequence)\n",
    "    X = pad_sequences(array_embedding_sequences)\n",
    "    return X\n",
    "X_train = tokens_to_embedding_sequences(train_array_token, model_sg_full)\n",
    "X_valid = tokens_to_embedding_sequences(valid_array_token, model_sg_full)\n",
    "X_test = tokens_to_embedding_sequences(test_array_token, model_sg_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.LSTM(units=256 ,activation=\"relu\", input_shape=(28, 300)))\n",
    "model.add(kl.Dense(256))\n",
    "model.add(kl.Activation(\"relu\"))\n",
    "model.add(kl.Dense(N_label))\n",
    "model.add(kl.Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "epochs = 500\n",
    "batch_size=256\n",
    "history = model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, Y_train_int, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=[X_valid, Y_valid_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
