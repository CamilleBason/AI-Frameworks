{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wikistat/AI-Frameworks/blob/master/RecomendationSystem/tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IA Frameworks](https://github.com/wikistat/AI-Frameworks) - Introduction to Recommendation System with Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Collaborative Filtering with `Tensorflow` Python Library.\n",
    "\n",
    "The objectives of this notebook are the following : \n",
    "\n",
    "* Build `Keras` models to learn embedding space for user and item data.\n",
    "* Visualize these spaces.\n",
    "* Use results of algorithm to apply recommendation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & Data (Google Colab)\n",
    "\n",
    "If you're running this notebook on Google colab, you do not have access to the `solutions` folder you get by cloning the repository locally. \n",
    "\n",
    "The following lines will allow you to build the folders and the files you need for this TP.\n",
    "\n",
    "**WARNING 1** Do not run this line localy. <br>\n",
    "**WARNING 2** The magic command `%load` does not work work on google colab, you will have to copy-paste the solution on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir solutions\n",
    "! wget -P solutions https://github.com/wikistat/AI-Frameworks/raw/master/RecomendationSystem/solutions/deep_recommender_model.py\n",
    "! wget -P solutions https://github.com/wikistat/AI-Frameworks/raw/master/RecomendationSystem/solutions/compile_fit.py\n",
    "! wget -P solutions https://github.com/wikistat/AI-Frameworks/raw/master/RecomendationSystem/solutions/top_10_closest_items.py\n",
    "! wget -P solutions https://github.com/wikistat/AI-Frameworks/raw/master/RecomendationSystem/solutions/top10_recommendation\n",
    "! mkdir movielens_small\n",
    "https://github.com/wikistat/AI-Frameworks/raw/master/RecomendationSystem/movielens_small/movies.csv\n",
    "https://github.com/wikistat/AI-Frameworks/raw/master/RecomendationSystem/movielens_small/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.models as km\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.decomposition as sdec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We use the same `movielens_small`dataset used in the first notebook `surprise.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"movielens_small/\"\n",
    "rating = pd.read_csv(DATA_DIR + \"ratings.csv\")\n",
    "nb_entries = rating.shape[0]\n",
    "print(\"Number of entries : %d \" %nb_entries)\n",
    "rating.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create two new columns. The column **user_id** (resp. **item_id**) re-arange the userId (resp. MovieId) columns in order that these columns lies in the range(0,609) (resp. range(0,0723))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdToNormUserId = {k:v for v,k in enumerate(rating.userId.unique())}\n",
    "rating[\"user_id\"] = [userIdToNormUserId[x] for x in rating.userId.values]\n",
    "itemIdToNormItemId = {k:v for v,k in enumerate(rating.movieId.unique())}\n",
    "rating[\"item_id\"] = [itemIdToNormItemId[x] for x in rating.movieId.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(DATA_DIR + \"movies.csv\")\n",
    "id_movie_to_title = dict(movies[[\"movieId\",\"title\"]].values)\n",
    "id_item_to_title = {itemIdToNormItemId[k]:v for k,v in id_movie_to_title.items() if k in itemIdToNormItemId}\n",
    "print(\"Number of movies in the dictionary : %d\" %(len(id_item_to_title)))\n",
    "rating[\"movie\"] = [id_movie_to_title[x] for x in rating[\"movieId\"].values]\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the same train/test dataset that the one in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_train,rating_test = train_test_split(rating, test_size=0.1, random_state=42)\n",
    "print(\"N train rates : %d\"%rating_train.shape[0])\n",
    "print(\"N test rates : %d\"%rating_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Recommender System\n",
    "\n",
    "We first build a very simple recommender according to this architecture:\n",
    "\n",
    "![alt text](images/simple_architecture.png)\n",
    "\n",
    "Let's decompose the construction of this network.\n",
    "\n",
    "We first create the inputs layer, which will take as entry the id of the user and the id of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample we input the integer identifiers of a single user and a single item\n",
    "user_id_input = kl.Input(shape=[1], name='user')\n",
    "item_id_input = kl.Input(shape=[1], name='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This id we will then be converted in their embedding space. This can be easily done with the `Embedding` layer object of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_id= rating.user_id.max()\n",
    "max_item_id= rating.item_id.max()\n",
    "embedding_size = 30\n",
    "user_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the dot product of the two vectors which are the vectors representation in the embedding space of the user and the item given in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from shape: (batch_size, input_length, embedding_size)\n",
    "# to shape: (batch_size, input_length * embedding_size) which is\n",
    "# equal to shape: (batch_size, embedding_size)\n",
    "user_vecs = kl.Flatten()(user_embedding)\n",
    "item_vecs = kl.Flatten()(item_embedding)\n",
    "\n",
    "y = kl.Dot(axes=1)([user_vecs, item_vecs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the objective and the architecture of the model above compile and fit this model using keras function. \n",
    "* What are the appropriate loss function?\n",
    "* What are x and y here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=)\n",
    "history = model.fit(x=, y=,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/compile_fit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction can now be applied by giving the list of user and item ids that we want to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % sm.mean_squared_error(train_preds, rating_train.rating))\n",
    "print(\"Final train MAE: %0.3f\" % sm.mean_absolute_error(train_preds, rating_train.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_test = rating_test.user_id.values\n",
    "item_id_test = rating_test.item_id.values\n",
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % sm.mean_squared_error(test_preds, rating_test.rating))\n",
    "print(\"Final test MAE: %0.3f\" % sm.mean_absolute_error(test_preds, rating_test.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What do you think about those results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep recommender model\n",
    "\n",
    "Let's know compute a deeper architecture in order to improve those results.\n",
    "\n",
    "![alt text](images/deep_architecture.png)\n",
    "\n",
    "\n",
    "**Exercise** : Implement a model similar to the previous one with:\n",
    "\n",
    "* A concatenate layer (look at the kl.Concatenate function)\n",
    "* A dropout layer (rate=0.5) after the concatenate layer.\n",
    "* only one Hidden layer with 64 neurons and relu activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/deep_recommender_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([user_id_train, item_id_train], rating_train.rating,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % sm.mean_squared_error(train_preds, rating_train.rating))\n",
    "print(\"Final train MAE: %0.3f\" % sm.mean_absolute_error(train_preds, rating_train.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % sm.mean_squared_error(test_preds, rating_test.rating))\n",
    "print(\"Final test MAE: %0.3f\" % sm.mean_absolute_error(test_preds, rating_test.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about those results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting the model\n",
    "\n",
    "In this section we will see how to explore both the model and the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar items and user.\n",
    "\n",
    "We want to find the K closest elements of an item or a user. The model we build can't be used directly as it takes into account a user and a item and not two users nor two items.\n",
    "\n",
    "But we can easily build a method based on the constructed embedding space. Let's first get the embedding matrices of the user and the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "user_embeddings = weights[0]\n",
    "print(\"User embedding matrix dimension : %s\" %str(user_embeddings.shape))\n",
    "item_embeddings = weights[1]\n",
    "print(\"item embedding matrix dimension : %s\" %str(item_embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** For an id of and item. Compute the distance (*cosine* or *euclidean*) of its embedding vector to all embedding vectors of the other items (Use `pairwise_distance`function from `sklearn`). \n",
    "\n",
    "Then display it's top 10 closest items (The procedure would be the same for the user, but the results are easier to interpreted with the movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/top_10_closest_items.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What do you think of these results?  Unfortunalty the dataset is to small to really get good meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Items\n",
    "\n",
    "The following function enable to display the projection of the items in a PCA decomposition space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaItems = sdec.PCA(n_components=2)\n",
    "items_pca_embeddings = pcaItems.fit_transform(item_embeddings)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(items_pca_embeddings[:,0], item_embeddings[:,1], linestyle=\"None\", marker=\".\")\n",
    "ax.plot(items_pca_embeddings[mostSimilarItem[2].values,0], item_embeddings[mostSimilarItem[2].values,1], linestyle=\"None\", marker=\".\", markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A recommendation function for a given user\n",
    "\n",
    "Once the model is trained, the system can be used to recommend a few items for a user, that he/she hasn't already seen:\n",
    "\n",
    "First let's select a user and display the movies he likes or dislikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "rating_user = rating[rating[\"user_id\"]==user_id]\n",
    "rating_user_sorted = rating_user.sort_values(\"rating\")\n",
    "print(\"10 best rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[-10:][[\"movie\",\"rating\"]])\n",
    "print(\"10 worst rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[:10][[\"movie\",\"rating\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Use the model to compute the estimated rates that the user would give to the movies he hasn't seen. Display the 10 movies you would recommend to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/top10_recommendation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL) Complete dataset\n",
    "\n",
    "The following code perform the same model on the complete dataset. \n",
    "It would take too much time if you don't have a GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"ml-25m/\"\n",
    "rating = pd.read_csv(DATA_DIR + \"ratings.csv\")\n",
    "nb_entries = rating.shape[0]\n",
    "print(\"Number of entries : %d \" %nb_entries)\n",
    "rating.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(DATA_DIR + \"movies.csv\")\n",
    "id_movie_to_title = dict(movies[[\"movieId\",\"title\"]].values)\n",
    "id_item_to_title = {itemIdToNormItemId[k]:v for k,v in id_movie_to_title.items() if k in itemIdToNormItemId}\n",
    "print(\"Number of movies in the dictionary : %d\" %(len(id_item_to_title)))\n",
    "rating[\"movie\"] = [id_movie_to_title[x] for x in rating[\"movieId\"].values]\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdToNormUserId = {k:v for v,k in enumerate(rating.userId.unique())}\n",
    "rating[\"user_id\"] = [userIdToNormUserId[x] for x in rating.userId.values]\n",
    "itemIdToNormItemId = {k:v for v,k in enumerate(rating.movieId.unique())}\n",
    "rating[\"item_id\"] = [itemIdToNormItemId[x] for x in rating.movieId.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_train,rating_test = train_test_split(rating, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input = kl.Input(shape=[1], name='user')\n",
    "item_id_input = kl.Input(shape=[1], name='item')\n",
    "\n",
    "embedding_size = 30\n",
    "max_user_id= rating.user_id.max()\n",
    "max_item_id= rating.item_id.max()\n",
    "user_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "# reshape from shape: (batch_size, input_length, embedding_size)\n",
    "# to shape: (batch_size, input_length * embedding_size) which is\n",
    "# equal to shape: (batch_size, embedding_size)\n",
    "user_vecs = kl.Flatten()(user_embedding)\n",
    "item_vecs = kl.Flatten()(item_embedding)\n",
    "\n",
    "input_vecs = kl.Concatenate()([user_vecs, item_vecs])\n",
    "input_vecs = kl.Dropout(0.5)(input_vecs)\n",
    "\n",
    "x = kl.Dense(64, activation='relu')(input_vecs)\n",
    "y = kl.Dense(1)(x)\n",
    "\n",
    "model = km.Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_train = rating_train.user_id.values\n",
    "item_id_train = rating_train.item_id.values\n",
    "history = model.fit([user_id_train, item_id_train], rating_train.rating,\n",
    "                    batch_size=2048, epochs=1, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "user_embeddings = weights[0]\n",
    "print(\"User embedding matrix dimension : %s\" %str(user_embeddings.shape))\n",
    "item_embeddings = weights[1]\n",
    "print(\"item embedding matrix dimension : %s\" %str(item_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 283\n",
    "X = np.expand_dims(item_embeddings[idx],axis=0)\n",
    "distX = sm.pairwise_distances(X, item_embeddings, metric=\"cosine\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 items similar to movies %s\" %str(id_item_to_title[idx]))\n",
    "mostSimilarItem = pd.DataFrame([[id_item_to_title[x], distX[x],x] for x in distX.argsort()[:10]])\n",
    "mostSimilarItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaItems = sdec.PCA(n_components=2)\n",
    "items_pca_embeddings = pcaItems.fit_transform(item_embeddings)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(items_pca_embeddings[:,0], item_embeddings[:,1], linestyle=\"None\", marker=\".\")\n",
    "ax.plot(items_pca_embeddings[mostSimilarItem[2].values,0], item_embeddings[mostSimilarItem[2].values,1], linestyle=\"None\", marker=\".\", markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "rating_user = rating[rating[\"user_id\"]==user_id]\n",
    "rating_user_sorted = rating_user.sort_values(\"rating\")\n",
    "print(\"10 best rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[-10:][[\"movie\",\"rating\"]])\n",
    "print(\"10 worst rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[:10][[\"movie\",\"rating\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run prediction for all movies\n",
    "prediction = model.predict([[user_id for _ in range(max_item_id)], [x for x in range(max_item_id)]])\n",
    "#Concatenate results with id of the movie\n",
    "prediction_with_id = zip(prediction, [x for x in range(max_item_id)])\n",
    "# Filter on unseen movie, get the title and sort the results according to predicted rate\n",
    "prediction_of_unseen_movie = sorted([[p[0],id_item_to_title[x]] for p,x in prediction_with_id if not(x in seen_movie)], key=lambda x :x[0], reverse = True)\n",
    "#Display it.\n",
    "pd.DataFrame(prediction_of_unseen_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[user_id for _ in range(max_item_id)], [x for x in range(max_item_id)]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
